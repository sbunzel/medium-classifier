{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:15:28.130946Z",
     "start_time": "2019-01-07T21:15:28.116918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# file system navigation\n",
    "from pathlib import Path\n",
    "\n",
    "# data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ml algorithms and evaluation metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import scipy\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "# sklearn specifics\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.util import decaying\n",
    "\n",
    "# keras\n",
    "from keras import datasets\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# misc\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:14:56.008186Z",
     "start_time": "2019-01-07T21:14:56.000107Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:14:56.039095Z",
     "start_time": "2019-01-07T21:14:56.011774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tf_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:14:56.055556Z",
     "start_time": "2019-01-07T21:14:56.050468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path.cwd() / \"data\" / \"shared\"\n",
    "OUTPUT_PATH = Path.cwd() / \"reports\" / \"images-and-tables\"\n",
    "MODEL_PATH = Path.cwd() / \"models\"\n",
    "SEED = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:14:56.068904Z",
     "start_time": "2019-01-07T21:14:56.060182Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TEXT_COL = \"text\"\n",
    "NUM_COLS = [\"claps\", \"reading_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:14:56.083663Z",
     "start_time": "2019-01-07T21:14:56.073698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not OUTPUT_PATH.is_dir():\n",
    "    OUTPUT_PATH.mkdir(parents=True)\n",
    "    \n",
    "if not MODEL_PATH.is_dir():\n",
    "    MODEL_PATH.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Helper visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Interest in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate a plot of interest in Machine Learning over time based on Google trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml = pd.read_csv(Path.cwd() / \"data\" / \"trends-ml.csv\",\n",
    "                               skiprows=3,\n",
    "                               header=None,\n",
    "                               names=[\"date\", \"interest\"],\n",
    "                               parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.749Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.755Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interest_plot = google_trends_ml.plot(x=\"date\",\n",
    "                      y=\"interest\",\n",
    "                      legend=False)\n",
    "interest_plot.set_xlabel(\"Date\")\n",
    "interest_plot.set_ylabel(\"Relative interest\")\n",
    "interest_plot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = interest_plot.get_figure()\n",
    "fig.savefig(OUTPUT_PATH / \"interest-in-ml.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load preprocessed data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:31:23.615211Z",
     "start_time": "2019-01-07T21:31:23.420383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(DATA_PATH / \"train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = data[[\"claps\", \"reading_time\", \"text\"]]\n",
    "y = np.array(data[\"interesting\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3, random_state=SEED,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Initialize custom evaluator and shuffle split generator to use across all modeling approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator = CustomEvaluator(target_precision=0.8)\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=6, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Generate a vocabulary of words specific to the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use a list of the top 10k most frequent words in the English language obtained from [this repository](https://github.com/first20hours/google-10000-english) to identify the words specific to the corpus of block posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TOP_WORDS_PATH = Path.cwd() / \"resources\" / \"top_words\"\n",
    "top_10k = pd.read_table(TOP_WORDS_PATH / \"google-10000-english\" / \"google-10000-english.txt\", header=None)\n",
    "top_10k_dict = {str(word).lower() : rank + 1 for rank, word in top_10k.iloc[:, 0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.799Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = data[\"text\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.806Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = texts.apply(lambda x: x.lower())\n",
    "texts = texts.apply(lambda x: clean_apostrophe(x))\n",
    "texts = texts.apply(lambda x: remove_punctuation(x))\n",
    "texts = texts.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = texts.apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "oov = check_coverage(vocab, top_10k_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.818Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "specific_vocab = [w for w, _ in oov]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Take a first look at the data and generate summary tables for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.826Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.837Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[\"interesting\"].value_counts() / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.854Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_numeric = (data[[\"claps\", \"reading_time\"]]\n",
    "                   .describe()\n",
    "                   .round(2))\n",
    "summary_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_numeric, name=\"summary_numeric\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.867Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_object = data[[\"author\", \"title\", \"text\"]].describe()\n",
    "summary_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.872Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_object, name=\"summary_object\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Distribution of target with respect to numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.883Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_base = data[[\"claps\", \"reading_time\", \"interesting\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.889Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_index = 0\n",
    "y_index = 1\n",
    "target_names = [\"not interesting\", \"interesting\"]\n",
    "\n",
    "colors = [\"red\", \"green\"]\n",
    "\n",
    "for label, color in zip(range(len(data_base[\"interesting\"])), colors):\n",
    "    plt.scatter(np.array(data_base[data_base[\"interesting\"]==label].iloc[:, x_index]), \n",
    "                np.array(data_base[data_base[\"interesting\"]==label].iloc[:, y_index]),\n",
    "                label=target_names[label],\n",
    "                c=color)\n",
    "\n",
    "plt.xlabel(data_base.columns[x_index])\n",
    "plt.ylabel(data_base.columns[y_index])\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.savefig(OUTPUT_PATH / \"base_classifier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Distribution of blog post lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:34:14.893162Z",
     "start_time": "2019-01-07T21:34:14.450831Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_words = [len(post.split(\" \")) for post in data[\"text\"]]\n",
    "words_per_review = len(data) / np.median(n_words)\n",
    "plt.hist(n_words, 30)\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Number of blog posts\")\n",
    "plt.xlim(0, 8000)\n",
    "plt.title(f\"Blog post number of words distribution (ratio: {words_per_review:.2f})\")\n",
    "plt.savefig(OUTPUT_PATH / \"blog-post-length.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:31:42.575419Z",
     "start_time": "2019-01-07T21:31:42.567324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Summary statistics on the number of words:\")\n",
    "print(f\"   Median: {np.median(n_words):.2f}\")\n",
    "print(f\"   Mean: {np.mean(n_words):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:31:43.197080Z",
     "start_time": "2019-01-07T21:31:43.191051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples / median number of words in each post: {words_per_review:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### For comparison: Distribution length for imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:20:13.682535Z",
     "start_time": "2019-01-07T21:20:03.488150Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imdb = datasets.imdb\n",
    "(train_imdb, _), (test_imdb, _) = imdb.load_data(Path.cwd() / \"data\" / \"imdb.npz\", num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:22:12.103039Z",
     "start_time": "2019-01-07T21:22:12.096286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imdb_full = np.concatenate((train_imdb, test_imdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:34:18.857125Z",
     "start_time": "2019-01-07T21:34:18.291237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_words_imdb = [len(review) for review in imdb_full]\n",
    "words_per_review_imdb = imdb_full.shape[0] / np.median(n_words_imdb)\n",
    "plt.hist(n_words, 50)\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Number of review\")\n",
    "plt.xlim(0, 8000)\n",
    "plt.title(f\"Imdb review number of words distribution (ratio: {words_per_review_imdb:.2f})\")\n",
    "plt.savefig(OUTPUT_PATH / \"imdb-review-length.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:24:56.562281Z",
     "start_time": "2019-01-07T21:24:56.538051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Summary statistics on the number of words for imdb:\")\n",
    "print(f\"   Median: {np.median(n_words_imdb):.2f}\")\n",
    "print(f\"   Mean: {np.mean(n_words_imdb):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T21:25:32.786242Z",
     "start_time": "2019-01-07T21:25:32.776549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples / median number of words in each review: {words_per_review_imdb:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a baseline model using just the numerical features `claps` and `reading time` based on three classes of classification models:\n",
    "\n",
    "- Random forests\n",
    "- Support vector machines\n",
    "- Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_num, X_test_num = np.array(X_train[num_cols]), np.array(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.917Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20,\n",
    "                            min_samples_leaf=3,\n",
    "                            random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.921Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_rfs = fit_ensemble(rf, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_rf = evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.941Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"auto\",\n",
    "          probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_svcs = fit_ensemble(svc, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.954Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_svc = evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_svc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\", random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.975Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_lrs = fit_ensemble(lr, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.982Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.988Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_lr = evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Collect and save baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.993Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results = pd.concat([base_results_rf, base_results_svc, base_results_lr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:36.999Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.004Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=base_results, name=\"summary_baseline_results\", out=OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Text based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Prepare feature array for training text based models by extracting just the column containing the blog posts' text from `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.010Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_text, X_test_text = np.array(X_train[TEXT_COL]), np.array(X_test[TEXT_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### CountVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scikit-learn's `CountVectorizer` is the simplest approach to turning the blog posts' texts into numerical matrices. It will just count the number of occurences of each token in the text and create a sparse matrix holding these counts for all posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let's do everything with default values to get a general feeling for how this approach performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.017Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer_specific = CountVectorizer(vocabulary=specific_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf = make_pipeline(count_vectorizer, RandomForestClassifier(n_estimators=10,\n",
    "                                                                          random_state=SEED,\n",
    "                                                                          n_jobs=-1))\n",
    "fitted_countvec_rf = fit_ensemble(pipe_countvec_rf, sss, X_train_text, y_train)\n",
    "res_countvec_rf_full = evaluate_ensemble(fitted_countvec_rf, evaluator, X_test_text, y_test,\n",
    "                                         return_res=True, method=f\"countvec rf full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_rf_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only top k words specific to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf_specific = make_pipeline(count_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                            random_state=1,\n",
    "                                                                                            n_jobs=-1))\n",
    "fitted_countvec_rf_specific = fit_ensemble(pipe_countvec_rf_specific, sss, X_train_text, y_train)\n",
    "res_countvec_rf_specific = evaluate_ensemble(fitted_countvec_rf_specific, evaluator, X_test_text, y_test,\n",
    "                                             return_res=True, method=f\"countvec rf specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.049Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_rf_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use scikit-learn's `StandardScaler` here as the SVC's default kernel (`rbf`) expects normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.057Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc = make_pipeline(count_vectorizer, StandardScaler(with_mean=False),\n",
    "                                  SVC(gamma=\"auto\", probability=True, random_state=SEED))\n",
    "fitted_countvec_svc = fit_ensemble(pipe_countvec_svc, sss, X_train_text, y_train)\n",
    "res_countvec_svc_full = evaluate_ensemble(fitted_countvec_svc, evaluator, X_test_text, y_test,\n",
    "                                          return_res=True, method=f\"countvec svc full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.062Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.068Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc_specific = make_pipeline(count_vectorizer_specific,\n",
    "                                           StandardScaler(with_mean=False),\n",
    "                                           SVC(gamma=\"auto\", probability=True, random_state=SEED))\n",
    "fitted_countvec_svc_specific = fit_ensemble(pipe_countvec_svc_specific, sss, X_train_text, y_train)\n",
    "res_countvec_svc_specific = evaluate_ensemble(fitted_countvec_svc_specific, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"countvec svc specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.073Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Grid search on best default models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Full vocab + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer()),\n",
    "    (\"std\", StandardScaler(with_mean=False)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=20000),\n",
    "          \"svc__C\": scipy.stats.expon(scale=1.0),\n",
    "          \"svc__gamma\": [\"auto\", \"scale\"],\n",
    "          \"svc__kernel\": [\"rbf\"],\n",
    "          \"svc__class_weight\": [\"balanced\", None]}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=50,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.086Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_fitted = grid.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.092Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(grid_fitted.best_estimator_, MODEL_PATH / \"countvec_full_svc_grid_best.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_full_grid_best = joblib.load(MODEL_PATH / \"countvec_full_svc_grid_best.pkl\")\n",
    "countvec_svc_full_grid_best = countvec_svc_full_grid_best.set_params(svc__random_state=SEED)\n",
    "fitted_countvec_svc_full_grid_best = fit_ensemble(countvec_svc_full_grid_best, sss,\n",
    "                                                X_train_text, y_train, print_progress=True)\n",
    "res_countvec_svc_full_best = evaluate_ensemble(fitted_countvec_svc_full_grid_best, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"countvec svc full best params v{SEED}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_full_grid_best.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.109Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_full_best, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Specific vocab + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer(vocabulary=specific_vocab)),\n",
    "    (\"std\", StandardScaler(with_mean=False)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=20000),\n",
    "          \"svc__C\": scipy.stats.expon(scale=1.0),\n",
    "          \"svc__gamma\": [\"auto\", \"scale\"],\n",
    "          \"svc__kernel\": [\"rbf\"],\n",
    "          \"svc__class_weight\": [\"balanced\", None]}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=50,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.122Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_fitted = grid.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(grid_fitted.best_estimator_, MODEL_PATH / \"countvec_specific_svc_grid_best.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.134Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_specific_grid_best = joblib.load(MODEL_PATH / \"countvec_specific_svc_grid_best.pkl\")\n",
    "countvec_svc_specific_grid_best = countvec_svc_specific_grid_best.set_params(svc__random_state=SEED)\n",
    "fitted_countvec_svc_specific_grid_best = fit_ensemble(countvec_svc_specific_grid_best, sss,\n",
    "                                                X_train_text, y_train, print_progress=True)\n",
    "res_countvec_svc_specific_best = evaluate_ensemble(fitted_countvec_svc_specific_grid_best, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"best params countvec svc specific v{SEED}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.140Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_specific_grid_best.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.146Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_specific_best, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Create summary of grid search results and save to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_countvec_grid = read_results(OUTPUT_PATH / \"raw\", \"best_params_countvec*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.157Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = pd.concat(list_countvec_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.161Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_groupby_method = res.groupby(\"base_method\")\n",
    "res_analysis_table = res_groupby_method.agg({c : [\"mean\", \"std\"] for c in [\"mean train auc\", \"mean cv auc\", \"mean test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = res_analysis_table.sort_values([(\"mean test auc\", \"mean\"), (\"mean cv auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.176Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"cv auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.182Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.187Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(out, \"summary_countvec_svc_best_params\", out=OUTPUT_PATH, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### TfidfVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next approach to feature extraction from the text data that I want to try is the `Term-Frequency-Inverse-Document-Frequency` technique implemented in scikit-learn's `TfidfVectorizer`. This method creates the same matrix as the `CountVectorizer` but divides the values for each token in the vocabulary by its frequency across all documents in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer_specific = TfidfVectorizer(vocabulary=specific_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.200Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf = make_pipeline(tfidf_vectorizer, RandomForestClassifier(n_estimators=10,\n",
    "                                                                    random_state=SEED,\n",
    "                                                                    n_jobs=-1))\n",
    "fitted_tfidf = fit_ensemble(pipe_tfidf, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_full = evaluate_ensemble(fitted_tfidf, evaluator, X_test_text, y_test,\n",
    "                                      return_res=True, method=f\"tfidf rf full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.205Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_rf_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only specific vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.210Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_specific = make_pipeline(tfidf_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                      random_state=SEED,\n",
    "                                                                                      n_jobs=-1))\n",
    "fitted_tfidf_specific = fit_ensemble(pipe_tfidf_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_specific = evaluate_ensemble(fitted_tfidf_specific, evaluator, X_test_text, y_test,\n",
    "                                          return_res=True, method=f\"tfidf rf specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.215Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_rf_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.221Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc = make_pipeline(tfidf_vectorizer,\n",
    "                               StandardScaler(with_mean=False),\n",
    "                               SVC(gamma=\"auto\",\n",
    "                                   probability=True,\n",
    "                                   random_state=SEED))\n",
    "fitted_tfidf_svc = fit_ensemble(pipe_tfidf_svc, sss, X_train_text, y_train)\n",
    "res_tfidf_svc_full = evaluate_ensemble(fitted_tfidf_svc, evaluator, X_test_text, y_test,\n",
    "                                       return_res=True, method=f\"tfidf svc full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.226Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_svc_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc_specific = make_pipeline(tfidf_vectorizer_specific,\n",
    "                                        StandardScaler(with_mean=False),\n",
    "                                        SVC(gamma=\"auto\",\n",
    "                                            probability=True,\n",
    "                                            random_state=SEED))\n",
    "fitted_tfidf_svc_specific = fit_ensemble(pipe_tfidf_svc_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_svc_specific = evaluate_ensemble(fitted_tfidf_svc_specific, evaluator, X_test_text, y_test,\n",
    "                                           return_res=True, method=f\"tfidf svc specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.236Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_svc_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Collect results from running the above with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.243Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_countvec = read_results(OUTPUT_PATH / \"raw\", \"countvec*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.248Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_tfidf = read_results(OUTPUT_PATH / \"raw\", \"tfidf*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec = pd.concat([e for l in [list_countvec, list_tfidf] for e in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.258Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.265Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=results_tfidf_countvec.drop(\"base_method\", axis=1).sort_values([\"mean test auc\", \"mean cv auc\"], axis=0, ascending=False),\n",
    "          name=\"countvec_tfidf_results_default_all_appendix\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Summarize results and save overview table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.270Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = results_tfidf_countvec.sort_values([\"mean test auc\", \"mean cv auc\"], axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.276Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_groupby_method = res.groupby(\"base_method\")\n",
    "res_analysis_table = res_groupby_method.agg({c : [\"mean\", \"std\"] for c in [\"mean train auc\", \"mean cv auc\", \"mean test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.282Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = res_analysis_table.sort_values([(\"mean test auc\", \"mean\"), (\"mean cv auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"cv auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.293Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=out,\n",
    "          name=\"summary_countvec_tfidf_results_default\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Pretrained word embeddings + neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The below is heavily based on these two Kaggle Kernels:\n",
    "- [Processing text when using word embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook)\n",
    "- [Comparing word embeddings](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.306Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_len = data[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "text_para_len = data[\"text\"].apply(lambda x: len(x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.312Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(vocab)} distinct words in the blog posts.\")\n",
    "print()\n",
    "print(\"Summary statistics on the number of words across all blog posts:\")\n",
    "print(f\"   Mean: {text_len.mean():.2f}\")\n",
    "print(f\"   Median: {text_len.median():.2f}\")\n",
    "print(f\"   Maximum: {text_len.max()}\")\n",
    "print(f\"   Minimum: {text_len.min()}\")\n",
    "print(f\"   Standard deviation: {text_len.std():.2f}\")\n",
    "print()\n",
    "print(\"Summary statistics on the number of paragraphs across all blog posts:\")\n",
    "print(f\"   Mean: {text_para_len.mean():.2f}\")\n",
    "print(f\"   Median: {text_para_len.median():.2f}\")\n",
    "print(f\"   Maximum: {text_para_len.max()}\")\n",
    "print(f\"   Minimum: {text_para_len.min()}\")\n",
    "print(f\"   Standard deviation: {text_para_len.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.317Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_LEN = 1000 # max number of words in a blog post\n",
    "EMBEDDING_PATH = Path.cwd() / \"resources\" / \"embeddings\"\n",
    "EMBEDDING_FOLDER = EMBEDDING_PATH / \"glove.840B.300d\"\n",
    "SPECIFIC_ONLY = False\n",
    "L2_REG = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Read embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (EMBEDDING_FOLDER / \"embeddings_index.pkl\").is_file():\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"rb\") as handle:\n",
    "        embeddings_index = pickle.load(handle)\n",
    "else:\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FOLDER / \"glove.840B.300d.txt\"))\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"wb\") as handle:\n",
    "        pickle.dump(embeddings_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Preprocess the text data to work well with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train_text.copy())\n",
    "X_test = pd.Series(X_test_text.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.340Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: x.lower())\n",
    "X_train = X_train.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_train.apply(lambda x: fix_punctuation(x))\n",
    "X_train = X_train.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Apply the same transformations to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.apply(lambda x: x.lower())\n",
    "X_test = X_test.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_test.apply(lambda x: fix_punctuation(x))\n",
    "X_test = X_test.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Tokenize the texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.354Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.360Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.368Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Process embeddings into a matrix of size `(max_features, embed_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.375Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.383Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "if SPECIFIC_ONLY: word_index = {word : i for word, i in word_index.items() if word in specific_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_FEATURES, len(tokenizer.word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "reg = regularizers.l2(L2_REG) if L2_REG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.408Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAX_LEN,))\n",
    "x = Embedding(MAX_FEATURES, embed_size, weights=[embedding_matrix], embeddings_regularizer=reg)(inp)\n",
    "x = Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=reg))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T22:20:54.207153Z",
     "start_time": "2018-11-26T22:20:54.193978Z"
    },
    "hidden": true
   },
   "source": [
    "##### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.421Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=16, epochs=10,\n",
    "          validation_data=(X_test, y_test), callbacks=[EarlyStopping(min_delta=0.005, patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_auc = metrics.roc_auc_score(y_train, model.predict(X_train))\n",
    "test_auc = metrics.roc_auc_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.433Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_keras = pd.DataFrame({\"method\": f\"embeddings keras v{SEED}\", \"train auc\": train_auc, \"test auc\": test_auc},\n",
    "                             index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(results_keras, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Collect results and save for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_keras = read_results(OUTPUT_PATH / \"raw\", \"*keras*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.460Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_results_keras = pd.concat(list_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.466Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_results_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.473Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=full_results_keras.drop(\"base_method\", axis=1).sort_values([\"test auc\"], axis=0, ascending=False),\n",
    "          name=\"keras_results_all_appendix\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Aggregate results and save for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.481Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = full_results_keras.groupby(\"base_method\").agg({c : [\"mean\", \"std\"] for c in [\"train auc\", \"test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = res.sort_values([(\"test auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.491Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.499Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.503Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=out,\n",
    "          name=\"keras_results_summary\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Language model + neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.datasets import URLs\n",
    "from fastai.text import TextList\n",
    "from fastai.basic_data import DatasetType\n",
    "from fastai.text.learner import text_classifier_learner\n",
    "from fastai.text.learner import language_model_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.516Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = MODEL_PATH / \"fastai\"\n",
    "if not PATH.is_dir():\n",
    "    PATH.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MIN_N_TOKENS = 0\n",
    "BPTT = 1000\n",
    "MAX_LEN = 2000\n",
    "SPLIT_TEXTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Optionally, we split the long texts into chunks based on new line characters, train on those and put the results back together in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_full = pd.DataFrame({\"text\": X_train_text, \"label\": y_train})\n",
    "X_test_full = pd.DataFrame({\"text\": X_test_text, \"label\": y_test})\n",
    "\n",
    "if SPLIT_TEXTS:\n",
    "    X_train_full_exploded = explode_texts(X_train_full)\n",
    "    X_test_full_exploded = explode_texts(X_test_full)\n",
    "\n",
    "    X_train_full_exploded[\"is_test\"] = False\n",
    "    X_test_full_exploded[\"is_test\"] = True\n",
    "\n",
    "    data_proc = pd.concat([X_train_full_exploded, X_test_full_exploded], axis=0)\n",
    "else:\n",
    "    X_train_full[\"is_test\"] = False\n",
    "    X_test_full[\"is_test\"] = True\n",
    "    data_proc = pd.concat([X_train_full, X_test_full], axis=0)\n",
    "    \n",
    "n_tokens = data_proc[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "print(\"Summary statistics on the number of words across all blog posts:\")\n",
    "print(f\"   Mean: {n_tokens.mean():.2f}\")\n",
    "print(f\"   Median: {n_tokens.median():.2f}\")\n",
    "print(f\"   Maximum: {n_tokens.max()}\")\n",
    "print(f\"   Minimum: {n_tokens.min()}\")\n",
    "print(f\"   Standard deviation: {n_tokens.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.540Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.547Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_proc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Train and Evaluate the Language Model (Metric : Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.553Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(df=data_proc, path=PATH, cols=\"text\")\n",
    "             .random_split_by_pct(0.2)\n",
    "             .label_for_lm()\n",
    "             .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, bptt=BPTT, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.565Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.571Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.577Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, lr, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"lm_fit_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.587Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"lm_fit_head\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.593Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, lr/10, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.599Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"lm_fine_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save_encoder(\"lm_fine_tuned_enc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Train and Evaluate the Classifier (Metric = ROC AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf_df = data_proc.copy()[n_tokens >= MIN_N_TOKENS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_clf = (TextList.from_df(df=clf_df, path=PATH, cols=[\"text\"], vocab=data_lm.vocab)\n",
    "               .split_from_df(\"is_test\")\n",
    "               .label_from_df(cols=\"label\")\n",
    "               .databunch(bs=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.631Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_clf.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clf, bptt=BPTT, max_len=MAX_LEN, drop_mult=0.3)\n",
    "learn.load_encoder(\"lm_fine_tuned_enc\")\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.647Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.652Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.657Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, lr, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.662Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_first_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_first_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.673Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(lr/(2.6**4), lr), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_second_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_second_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.690Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice((lr/5)/(2.6**4), lr/5), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_third_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.702Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_third_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.708Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice((lr/10)/(2.6**4), lr/10), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p1_test = clf_df.loc[clf_df.is_test == True, \"text\"].apply(lambda t: learn.predict(t)[2].numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.720Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p1_train = clf_df.loc[clf_df.is_test == False, \"text\"].apply(lambda t: learn.predict(t)[2].numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.727Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#This doesn't seem to produce results consistent with what we get when explicitly calling predict on each sample\n",
    "#I do need to finish this assignment before looking into this more closely\n",
    "#Thus, I will use the slower version above for now\n",
    "#p1_test = learn.get_preds()[0][:, 1].numpy()\n",
    "#p1_train = learn.get_preds(ds_type=DatasetType.Train)[0][:, 1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.734Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if SPLIT_TEXTS:\n",
    "    print(\"TRAIN PERFORMANCE\")\n",
    "    evaluate_exploded(clf_df[clf_df.is_test == False], p1_train, y_train)\n",
    "    print()\n",
    "    print(\"TEST PERFORMANCE\")\n",
    "    evaluate_exploded(clf_df[clf_df.is_test == True], p1_test, y_test)\n",
    "else:\n",
    "    print(\"TRAIN PERFORMANCE\")\n",
    "    train_auc = metrics.roc_auc_score(clf_df.loc[clf_df.is_test == False, \"label\"], p1_train)\n",
    "    print(f\"AUC: {train_auc:.2f}\")\n",
    "    print()\n",
    "    print(\"TEST PERFORMANCE\")\n",
    "    test_auc = metrics.roc_auc_score(clf_df.loc[clf_df.is_test == True, \"label\"], p1_test)\n",
    "    print(f\"AUC: {test_auc:.2f}\")\n",
    "    results_lm_finetune = pd.DataFrame({\"method\": f\"lm fine tuning v{SEED}\", \"train auc\": train_auc, \"test auc\": test_auc},\n",
    "                         index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.739Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_lm_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.744Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(results_lm_finetune, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Collect results and save for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.751Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_lm_finetune = read_results(OUTPUT_PATH, \"*lm*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_results_lm_finetune = pd.concat(list_lm_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_results_lm_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.768Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=full_results_lm_finetune.drop(\"base_method\", axis=1).sort_values([\"test auc\"], axis=0, ascending=False),\n",
    "          name=\"lm_finetune_results_all_appendix\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Summarize results and save summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.775Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = full_results_lm_finetune.groupby(\"base_method\").agg({c : [\"mean\", \"std\"] for c in [\"train auc\", \"test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.781Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = res.sort_values([(\"test auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.788Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=out,\n",
    "          name=\"lm_finetune_results_summary\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Google's guide for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The below is based on the [this guide](https://developers.google.com/machine-learning/guides/text-classification/) was attempted after all of the above had been experimented with. Google's guide to building a text classifier suggests a simple heuristic to decide whether to use n-gram based approaches (e.g. those illustrated in 6.1 and 6.2) or sequence based approaches (e.g. using recurrent neural networks as showcased in chapters 6.3 and 6.4):\n",
    "\n",
    "`if number of samples/number of words per sample < 1500 tokenize as n-grams and use an MLP (or boosted tree, SVM) to classifiy`\n",
    "\n",
    "`if > 1500 tokenize as sequences and use a sequence model (CNN, RNN) to classify, potentially building on pretrained word embeddings`\n",
    "\n",
    "As in our case the articles are rather long and we have just a small number of samples, we will follow the first approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Tokenize texts as unigrams and bigrams with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_text_vec, X_test_text_vec = ngram_vectorize(X_train_text, y_train, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Build and train an MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.815Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_in = ((X_train_text, y_train), (X_test_text, y_test))\n",
    "model = tf_helpers.train_ngram_model(data_in,\n",
    "                          batch_size=8,\n",
    "                          dropout_rate=0.3,\n",
    "                          learning_rate=5e-4,\n",
    "                          units=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_train, model.predict(X_train_text_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, model.predict(X_test_text_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Final evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Run for different random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run best model for different seeds and save resulting auc, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.835Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_text, X_test_text = np.array(X_train[TEXT_COL]), np.array(X_test[TEXT_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.840Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_model = joblib.load(MODEL_PATH / \"countvec_full_svc_grid_best.pkl\").set_params(svc__random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.851Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_best = get_best_scores(y_test, best_model.predict_proba(X_test_text)[:, 1], f\"final_model_svc_countvec_full_v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_best, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run best benchmark model as a point of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.868Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_num, X_test_num = np.array(X_train[NUM_COLS]), np.array(X_test[NUM_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\", random_state=SEED)\n",
    "lr.fit(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.879Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_base = get_best_scores(y_test, lr.predict_proba(X_test_num)[:, 1], f\"baseline_model_lr_v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.884Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.891Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_base, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Collect and summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_best = read_results(OUTPUT_PATH / \"raw\", \"final_model*.pkl\")\n",
    "full_res_best = pd.concat(list_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_res_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.909Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_base = read_results(OUTPUT_PATH / \"raw\", \"baseline_model*.pkl\")\n",
    "full_res_base = pd.concat(list_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.917Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_res_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.925Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = pd.concat([full_res_base, full_res_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res.drop(\"base_method\", axis=1).sort_values(\"f_0.8\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Save raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.939Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(res.drop(\"base_method\", axis=1).sort_values(\"f_0.8\", ascending=False), \"final_evaluation_raw_appendix\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Aggregate results and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary = (res\n",
    "          .groupby(\"base_method\")\n",
    "          .agg({c : [\"mean\", \"std\"] for c in [\"roc_auc\", \"precision\", \"recall\", \"f_0.8\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.955Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary.index.name = None\n",
    "index = pd.MultiIndex.from_product([[\"roc auc\", \"precision\", \"recall\", \"f_0.8\"], [\"mean\", \"std\"]])\n",
    "summary.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.970Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-07T21:14:37.977Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(summary.T, \"final_evaluation_summary\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:medium-classifier]",
   "language": "python",
   "name": "conda-env-medium-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
