{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:25.199496Z",
     "start_time": "2019-01-04T17:46:21.862836Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# file system navigation\n",
    "from pathlib import Path\n",
    "\n",
    "# data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ml algorithms and evaluation metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import scipy\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "# sklearn specifics\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.util import decaying\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# misc\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:25.210586Z",
     "start_time": "2019-01-04T17:46:25.202543Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:25.224079Z",
     "start_time": "2019-01-04T17:46:25.214888Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path.cwd() / \"data\" / \"shared\"\n",
    "OUTPUT_PATH = Path.cwd() / \"reports\" / \"images-and-tables\"\n",
    "MODEL_PATH = Path.cwd() / \"models\"\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:25.235712Z",
     "start_time": "2019-01-04T17:46:25.227633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not OUTPUT_PATH.is_dir():\n",
    "    OUTPUT_PATH.mkdir(parents=True)\n",
    "    \n",
    "if not MODEL_PATH.is_dir():\n",
    "    MODEL_PATH.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Helper visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate a plot of interest in Machine Learning over time based on Google trends data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:37.344338Z",
     "start_time": "2019-01-04T17:41:37.326121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml = pd.read_csv(Path.cwd() / \"data\" / \"trends-ml.csv\",\n",
    "                               skiprows=3,\n",
    "                               header=None,\n",
    "                               names=[\"date\", \"interest\"],\n",
    "                               parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:37.363697Z",
     "start_time": "2019-01-04T17:41:37.346854Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:37.584114Z",
     "start_time": "2019-01-04T17:41:37.367165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interest_plot = google_trends_ml.plot(x=\"date\",\n",
    "                      y=\"interest\",\n",
    "                      legend=False)\n",
    "interest_plot.set_xlabel(\"Date\")\n",
    "interest_plot.set_ylabel(\"Relative interest\")\n",
    "interest_plot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.310648Z",
     "start_time": "2019-01-04T17:41:37.587849Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = interest_plot.get_figure()\n",
    "fig.savefig(Path.cwd() / \"reports\" / \"images\" / \"interest-in-ml.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Load preprocessed data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:27.637730Z",
     "start_time": "2019-01-04T17:46:27.519566Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(DATA_PATH / \"train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:28.241868Z",
     "start_time": "2019-01-04T17:46:28.230940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = data[[\"claps\", \"reading_time\", \"text\"]]\n",
    "y = np.array(data[\"interesting\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3, random_state=SEED,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Initialize custom evaluator and shuffle split generator to use across all modeling approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:28.829073Z",
     "start_time": "2019-01-04T17:46:28.824232Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator = CustomEvaluator(target_precision=0.8)\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=6, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Generate a vocabulary of words specific to the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use a list of the top 10k most frequent words in the English language obtained from [this repository](https://github.com/first20hours/google-10000-english) to identify the words specific to the corpus of block posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:29.591178Z",
     "start_time": "2019-01-04T17:46:29.561515Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TOP_WORDS_PATH = Path.cwd() / \"resources\" / \"top_words\"\n",
    "top_10k = pd.read_table(TOP_WORDS_PATH / \"google-10000-english\" / \"google-10000-english.txt\", header=None)\n",
    "top_10k_dict = {str(word).lower() : rank + 1 for rank, word in top_10k.iloc[:, 0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:29.739265Z",
     "start_time": "2019-01-04T17:46:29.735179Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = data[\"text\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:30.074033Z",
     "start_time": "2019-01-04T17:46:29.908581Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = texts.apply(lambda x: x.lower())\n",
    "texts = texts.apply(lambda x: clean_apostrophe(x))\n",
    "texts = texts.apply(lambda x: remove_punctuation(x))\n",
    "texts = texts.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:30.264819Z",
     "start_time": "2019-01-04T17:46:30.106760Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = texts.apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "oov = check_coverage(vocab, top_10k_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:30.831874Z",
     "start_time": "2019-01-04T17:46:30.825366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "specific_vocab = [w for w, _ in oov]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Take a first look at the data and generate summary tables for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.325682Z",
     "start_time": "2019-01-04T17:41:33.384Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.327063Z",
     "start_time": "2019-01-04T17:41:33.390Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.328978Z",
     "start_time": "2019-01-04T17:41:33.400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[\"interesting\"].value_counts() / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.330649Z",
     "start_time": "2019-01-04T17:41:33.408Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_numeric = (data[[\"claps\", \"reading_time\"]]\n",
    "                   .describe()\n",
    "                   .round(2))\n",
    "summary_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.332626Z",
     "start_time": "2019-01-04T17:41:33.415Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_numeric, name=\"summary_numeric\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.334060Z",
     "start_time": "2019-01-04T17:41:33.421Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_object = data[[\"author\", \"title\", \"text\"]].describe()\n",
    "summary_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.336020Z",
     "start_time": "2019-01-04T17:41:33.428Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_object, name=\"summary_object\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Exploratory visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.337890Z",
     "start_time": "2019-01-04T17:41:33.437Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_base = data[[\"claps\", \"reading_time\", \"interesting\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.340783Z",
     "start_time": "2019-01-04T17:41:33.445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_index = 0\n",
    "y_index = 1\n",
    "target_names = [\"not interesting\", \"interesting\"]\n",
    "\n",
    "colors = [\"red\", \"green\"]\n",
    "\n",
    "for label, color in zip(range(len(data_base[\"interesting\"])), colors):\n",
    "    plt.scatter(np.array(data_base[data_base[\"interesting\"]==label].iloc[:, x_index]), \n",
    "                np.array(data_base[data_base[\"interesting\"]==label].iloc[:, y_index]),\n",
    "                label=target_names[label],\n",
    "                c=color)\n",
    "\n",
    "plt.xlabel(data_base.columns[x_index])\n",
    "plt.ylabel(data_base.columns[y_index])\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.savefig(Path.cwd() / \"reports\" / \"images\" / \"base_classifier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a baseline model using just the numerical features `claps` and `reading time` based on three classes of classification models:\n",
    "\n",
    "- Random forests\n",
    "- Support vector machines\n",
    "- Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.342766Z",
     "start_time": "2019-01-04T17:41:33.453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_cols = [\"claps\", \"reading_time\"]\n",
    "X_train_num, X_test_num = np.array(X_train[num_cols]), np.array(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.344600Z",
     "start_time": "2019-01-04T17:41:33.465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20,\n",
    "                            min_samples_leaf=3,\n",
    "                            random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.347297Z",
     "start_time": "2019-01-04T17:41:33.482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_rfs = fit_ensemble(rf, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.349921Z",
     "start_time": "2019-01-04T17:41:33.490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.352004Z",
     "start_time": "2019-01-04T17:41:33.497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_rf = evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.354025Z",
     "start_time": "2019-01-04T17:41:33.504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"auto\",\n",
    "          probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.356172Z",
     "start_time": "2019-01-04T17:41:33.510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_svcs = fit_ensemble(svc, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.358108Z",
     "start_time": "2019-01-04T17:41:33.515Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.360537Z",
     "start_time": "2019-01-04T17:41:33.521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_svc = evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_svc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.362524Z",
     "start_time": "2019-01-04T17:41:33.527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\", random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.364560Z",
     "start_time": "2019-01-04T17:41:33.532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_lrs = fit_ensemble(lr, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.366856Z",
     "start_time": "2019-01-04T17:41:33.537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.368857Z",
     "start_time": "2019-01-04T17:41:33.542Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_lr = evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Collect and save baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.371036Z",
     "start_time": "2019-01-04T17:41:33.547Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results = pd.concat([base_results_rf, base_results_svc, base_results_lr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.373677Z",
     "start_time": "2019-01-04T17:41:33.552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.375414Z",
     "start_time": "2019-01-04T17:41:33.558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=base_results, name=\"summary_baseline_results\", out=OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Text based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Prepare feature array for training text based models by extracting just the column containing the blog posts' text from `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:33.112235Z",
     "start_time": "2019-01-04T17:46:33.107212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "X_train_text, X_test_text = np.array(X_train[text_col]), np.array(X_test[text_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### CountVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scikit-learn's `CountVectorizer` is the simplest approach to turning the blog posts' texts into numerical matrices. It will just count the number of occurences of each token in the text and create a sparse matrix holding these counts for all posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let's do everything with default values to get a general feeling for how this approach performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.378301Z",
     "start_time": "2019-01-04T17:41:33.570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer_specific = CountVectorizer(vocabulary=specific_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.380712Z",
     "start_time": "2019-01-04T17:41:33.576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.382044Z",
     "start_time": "2019-01-04T17:41:33.585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf = make_pipeline(count_vectorizer, RandomForestClassifier(n_estimators=10,\n",
    "                                                                          random_state=SEED,\n",
    "                                                                          n_jobs=-1))\n",
    "fitted_countvec_rf = fit_ensemble(pipe_countvec_rf, sss, X_train_text, y_train)\n",
    "res_countvec_rf_full = evaluate_ensemble(fitted_countvec_rf, evaluator, X_test_text, y_test,\n",
    "                                         return_res=True, method=f\"countvec rf full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.384511Z",
     "start_time": "2019-01-04T17:41:33.591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_rf_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only top k words specific to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.387046Z",
     "start_time": "2019-01-04T17:41:33.596Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf_specific = make_pipeline(count_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                            random_state=1,\n",
    "                                                                                            n_jobs=-1))\n",
    "fitted_countvec_rf_specific = fit_ensemble(pipe_countvec_rf_specific, sss, X_train_text, y_train)\n",
    "res_countvec_rf_specific = evaluate_ensemble(fitted_countvec_rf_specific, evaluator, X_test_text, y_test,\n",
    "                                             return_res=True, method=f\"countvec rf specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.388855Z",
     "start_time": "2019-01-04T17:41:33.602Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_rf_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use scikit-learn's `StandardScaler` here as the SVC's default kernel (`rbf`) expects normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.391183Z",
     "start_time": "2019-01-04T17:41:33.608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc = make_pipeline(count_vectorizer, StandardScaler(with_mean=False),\n",
    "                                  SVC(gamma=\"auto\", probability=True, random_state=SEED))\n",
    "fitted_countvec_svc = fit_ensemble(pipe_countvec_svc, sss, X_train_text, y_train)\n",
    "res_countvec_svc_full = evaluate_ensemble(fitted_countvec_svc, evaluator, X_test_text, y_test,\n",
    "                                          return_res=True, method=f\"countvec svc full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.392940Z",
     "start_time": "2019-01-04T17:41:33.613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.395480Z",
     "start_time": "2019-01-04T17:41:33.618Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc_specific = make_pipeline(count_vectorizer_specific,\n",
    "                                           StandardScaler(with_mean=False),\n",
    "                                           SVC(gamma=\"auto\", probability=True, random_state=SEED))\n",
    "fitted_countvec_svc_specific = fit_ensemble(pipe_countvec_svc_specific, sss, X_train_text, y_train)\n",
    "res_countvec_svc_specific = evaluate_ensemble(fitted_countvec_svc_specific, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"countvec svc specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.397584Z",
     "start_time": "2019-01-04T17:41:33.623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Grid search on best default models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Full vocab + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.399651Z",
     "start_time": "2019-01-04T17:41:33.629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer()),\n",
    "    (\"std\", StandardScaler(with_mean=False)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=20000),\n",
    "          \"svc__C\": scipy.stats.expon(scale=1.0),\n",
    "          \"svc__gamma\": [\"auto\", \"scale\"],\n",
    "          \"svc__kernel\": [\"rbf\"],\n",
    "          \"svc__class_weight\": [\"balanced\", None]}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=50,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.401793Z",
     "start_time": "2019-01-04T17:41:33.635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_fitted = grid.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.404626Z",
     "start_time": "2019-01-04T17:41:33.641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(grid_fitted.best_estimator_, MODEL_PATH / \"countvec_full_svc_grid_best.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.407247Z",
     "start_time": "2019-01-04T17:41:33.647Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_full_grid_best = joblib.load(MODEL_PATH / \"countvec_full_svc_grid_best.pkl\")\n",
    "countvec_svc_full_grid_best = countvec_svc_full_grid_best.set_params(svc__random_state=SEED)\n",
    "fitted_countvec_svc_full_grid_best = fit_ensemble(countvec_svc_full_grid_best, sss,\n",
    "                                                X_train_text, y_train, print_progress=True)\n",
    "res_countvec_svc_full_best = evaluate_ensemble(fitted_countvec_svc_full_grid_best, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"countvec svc full best params v{SEED}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:59:03.343861Z",
     "start_time": "2019-01-04T17:59:03.330044Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_full_grid_best.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.413226Z",
     "start_time": "2019-01-04T17:41:33.658Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_full_best, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Specific vocab + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:42:18.705334Z",
     "start_time": "2019-01-04T17:42:18.690244Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer(vocabulary=specific_vocab)),\n",
    "    (\"std\", StandardScaler(with_mean=False)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=20000),\n",
    "          \"svc__C\": scipy.stats.expon(scale=1.0),\n",
    "          \"svc__gamma\": [\"auto\", \"scale\"],\n",
    "          \"svc__kernel\": [\"rbf\"],\n",
    "          \"svc__class_weight\": [\"balanced\", None]}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=50,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:43:22.605494Z",
     "start_time": "2019-01-04T17:42:24.012035Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_fitted = grid.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:44:10.925330Z",
     "start_time": "2019-01-04T17:44:10.767530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(grid_fitted.best_estimator_, MODEL_PATH / \"countvec_specific_svc_grid_best.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:47.470853Z",
     "start_time": "2019-01-04T17:46:43.867196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_specific_grid_best = joblib.load(MODEL_PATH / \"countvec_specific_svc_grid_best.pkl\")\n",
    "countvec_svc_specific_grid_best = countvec_svc_specific_grid_best.set_params(svc__random_state=SEED)\n",
    "fitted_countvec_svc_specific_grid_best = fit_ensemble(countvec_svc_specific_grid_best, sss,\n",
    "                                                X_train_text, y_train, print_progress=True)\n",
    "res_countvec_svc_specific_best = evaluate_ensemble(fitted_countvec_svc_specific_grid_best, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"best params countvec svc specific v{SEED}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:56:42.562406Z",
     "start_time": "2019-01-04T17:56:42.538454Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_specific_grid_best.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:50.376285Z",
     "start_time": "2019-01-04T17:46:50.362907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_specific_best, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Create summary of grid search results and save to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:55.905856Z",
     "start_time": "2019-01-04T17:46:55.874298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_countvec_grid = read_results(OUTPUT_PATH / \"raw\", \"best_params_countvec*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:47:49.569125Z",
     "start_time": "2019-01-04T17:47:49.562156Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = pd.concat(list_countvec_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:01.571932Z",
     "start_time": "2019-01-04T17:48:01.553694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_groupby_method = res.groupby(\"base_method\")\n",
    "res_analysis_table = res_groupby_method.agg({c : [\"mean\", \"std\"] for c in [\"mean train auc\", \"mean cv auc\", \"mean test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:13.400238Z",
     "start_time": "2019-01-04T17:48:13.391141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = res_analysis_table.sort_values([(\"mean test auc\", \"mean\"), (\"mean cv auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:22.758106Z",
     "start_time": "2019-01-04T17:48:22.750427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"cv auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:26.219536Z",
     "start_time": "2019-01-04T17:48:26.198324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:49:13.765812Z",
     "start_time": "2019-01-04T17:49:13.734172Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(out, \"summary_countvec_svc_best_params\", out=OUTPUT_PATH, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### TfidfVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next approach to feature extraction from the text data that I want to try is the `Term-Frequency-Inverse-Document-Frequency` technique implemented in scikit-learn's `TfidfVectorizer`. This method creates the same matrix as the `CountVectorizer` but divides the values for each token in the vocabulary by its frequency across all documents in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.464374Z",
     "start_time": "2019-01-04T17:41:33.776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer_specific = TfidfVectorizer(vocabulary=specific_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.466802Z",
     "start_time": "2019-01-04T17:41:33.780Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf = make_pipeline(tfidf_vectorizer, RandomForestClassifier(n_estimators=10,\n",
    "                                                                    random_state=SEED,\n",
    "                                                                    n_jobs=-1))\n",
    "fitted_tfidf = fit_ensemble(pipe_tfidf, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_full = evaluate_ensemble(fitted_tfidf, evaluator, X_test_text, y_test,\n",
    "                                      return_res=True, method=f\"tfidf rf full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.469504Z",
     "start_time": "2019-01-04T17:41:33.785Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_rf_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only specific vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.471668Z",
     "start_time": "2019-01-04T17:41:33.791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_specific = make_pipeline(tfidf_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                      random_state=SEED,\n",
    "                                                                                      n_jobs=-1))\n",
    "fitted_tfidf_specific = fit_ensemble(pipe_tfidf_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_specific = evaluate_ensemble(fitted_tfidf_specific, evaluator, X_test_text, y_test,\n",
    "                                          return_res=True, method=f\"tfidf rf specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.473718Z",
     "start_time": "2019-01-04T17:41:33.795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_rf_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.475636Z",
     "start_time": "2019-01-04T17:41:33.802Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc = make_pipeline(tfidf_vectorizer,\n",
    "                               StandardScaler(with_mean=False),\n",
    "                               SVC(gamma=\"auto\",\n",
    "                                   probability=True,\n",
    "                                   random_state=SEED))\n",
    "fitted_tfidf_svc = fit_ensemble(pipe_tfidf_svc, sss, X_train_text, y_train)\n",
    "res_tfidf_svc_full = evaluate_ensemble(fitted_tfidf_svc, evaluator, X_test_text, y_test,\n",
    "                                       return_res=True, method=f\"tfidf svc full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.477760Z",
     "start_time": "2019-01-04T17:41:33.808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_svc_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.479272Z",
     "start_time": "2019-01-04T17:41:33.813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc_specific = make_pipeline(tfidf_vectorizer_specific,\n",
    "                                        StandardScaler(with_mean=False),\n",
    "                                        SVC(gamma=\"auto\",\n",
    "                                            probability=True,\n",
    "                                            random_state=SEED))\n",
    "fitted_tfidf_svc_specific = fit_ensemble(pipe_tfidf_svc_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_svc_specific = evaluate_ensemble(fitted_tfidf_svc_specific, evaluator, X_test_text, y_test,\n",
    "                                           return_res=True, method=f\"tfidf svc specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.481017Z",
     "start_time": "2019-01-04T17:41:33.817Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_svc_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Collect results from running the above with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.483241Z",
     "start_time": "2019-01-04T17:41:33.823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_countvec = read_results(OUTPUT_PATH / \"raw\", \"countvec*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.485615Z",
     "start_time": "2019-01-04T17:41:33.829Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_tfidf = read_results(OUTPUT_PATH / \"raw\", \"tfidf*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.488827Z",
     "start_time": "2019-01-04T17:41:33.833Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec = pd.concat([e for l in [list_countvec, list_tfidf] for e in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.491447Z",
     "start_time": "2019-01-04T17:41:33.839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.493386Z",
     "start_time": "2019-01-04T17:41:33.844Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=results_tfidf_countvec.drop(\"base_method\", axis=1).sort_values([\"mean test auc\", \"mean cv auc\"], axis=0, ascending=False),\n",
    "          name=\"countvec_tfidf_results_default_all_appendix\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Summarize results and save overview table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.495388Z",
     "start_time": "2019-01-04T17:41:33.849Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = results_tfidf_countvec.sort_values([\"mean test auc\", \"mean cv auc\"], axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.497667Z",
     "start_time": "2019-01-04T17:41:33.856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_groupby_method = res.groupby(\"base_method\")\n",
    "res_analysis_table = res_groupby_method.agg({c : [\"mean\", \"std\"] for c in [\"mean train auc\", \"mean cv auc\", \"mean test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.499515Z",
     "start_time": "2019-01-04T17:41:33.862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = res_analysis_table.sort_values([(\"mean test auc\", \"mean\"), (\"mean cv auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.501450Z",
     "start_time": "2019-01-04T17:41:33.867Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"cv auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.503067Z",
     "start_time": "2019-01-04T17:41:33.873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.505035Z",
     "start_time": "2019-01-04T17:41:33.878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=out,\n",
    "          name=\"summary_countvec_tfidf_results_default\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Pretrained word embeddings + neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The below is heavily based on these two Kaggle Kernels:\n",
    "- [Processing text when using word embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook)\n",
    "- [Comparing word embeddings](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.519114Z",
     "start_time": "2019-01-04T17:41:33.923Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_LEN = 1000 # max number of words in a blog post\n",
    "EMBEDDING_PATH = Path.cwd() / \"resources\" / \"embeddings\"\n",
    "EMBEDDING_FOLDER = EMBEDDING_PATH / \"glove.840B.300d\"\n",
    "SPECIFIC_ONLY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Purpose specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.521659Z",
     "start_time": "2019-01-04T17:41:33.929Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Read embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.522893Z",
     "start_time": "2019-01-04T17:41:33.941Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.524328Z",
     "start_time": "2019-01-04T17:41:33.949Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (EMBEDDING_FOLDER / \"embeddings_index.pkl\").is_file():\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"rb\") as handle:\n",
    "        embeddings_index = pickle.load(handle)\n",
    "else:\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FOLDER / \"glove.840B.300d.txt\"))\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"wb\") as handle:\n",
    "        pickle.dump(embeddings_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Preprocess the text data to work well with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.525663Z",
     "start_time": "2019-01-04T17:41:33.959Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train_text.copy())\n",
    "X_test = pd.Series(X_test_text.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.526821Z",
     "start_time": "2019-01-04T17:41:33.965Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: x.lower())\n",
    "X_train = X_train.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_train.apply(lambda x: fix_punctuation(x))\n",
    "X_train = X_train.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Apply the same transformations to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.527953Z",
     "start_time": "2019-01-04T17:41:33.971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.apply(lambda x: x.lower())\n",
    "X_test = X_test.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_test.apply(lambda x: fix_punctuation(x))\n",
    "X_test = X_test.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Tokenize text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.529087Z",
     "start_time": "2019-01-04T17:41:33.978Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.530677Z",
     "start_time": "2019-01-04T17:41:33.983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.532026Z",
     "start_time": "2019-01-04T17:41:33.990Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Process embeddings into a matrix of size `(max_features, embed_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.533191Z",
     "start_time": "2019-01-04T17:41:33.997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.534466Z",
     "start_time": "2019-01-04T17:41:34.005Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "if SPECIFIC_ONLY: word_index = {word : i for word, i in word_index.items() if word in specific_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.535662Z",
     "start_time": "2019-01-04T17:41:34.012Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_FEATURES, len(tokenizer.word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.537104Z",
     "start_time": "2019-01-04T17:41:34.018Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAX_LEN,))\n",
    "x = Embedding(MAX_FEATURES, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T22:20:54.207153Z",
     "start_time": "2018-11-26T22:20:54.193978Z"
    },
    "hidden": true
   },
   "source": [
    "##### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.540711Z",
     "start_time": "2019-01-04T17:41:34.026Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.training import Model as keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.542139Z",
     "start_time": "2019-01-04T17:41:34.033Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_keras = fit_ensemble(model, sss, X_train, y_train, print_progress=True,\n",
    "                            batch_size=16, epochs=3, verbose=0, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.544249Z",
     "start_time": "2019-01-04T17:41:34.040Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=8,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.546108Z",
     "start_time": "2019-01-04T17:41:34.047Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.training import Model as keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.548226Z",
     "start_time": "2019-01-04T17:41:34.054Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_ensemble(fitted:List, eval:CustomEvaluator, X_test:ndarray,\n",
    "                      y_test:ndarray, return_res:bool=False, method:str=\"default\") -> pd.DataFrame:\n",
    "    \"\"\"Evaluate the performance of a set of classifiers trained on different subsets of the training set\n",
    "    \n",
    "    Arguments:\n",
    "    fitted - list of named tuples containing fitted models as well as their train and out of bag AUC\n",
    "    eval - object of class CustomEvaluator used to evaluate the performance on the hold out set\n",
    "    X_test - numpy array of the texts for the hold out set for final evaluation\n",
    "    y_test - numpy array of labels for the hold out set for final evaluation\n",
    "    \n",
    "    Return:\n",
    "    pd.DataFrame - if requested, return pandas dataframe summarizing the results\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(fitted[0], \"clf\"):\n",
    "        train_scores = [m.train_auc for m in fitted]\n",
    "        oob_scores = [m.oob_auc for m in fitted]\n",
    "        \n",
    "        if hasattr(fitted[0].clf, \"predict_proba\"):\n",
    "            preds_test = np.array([m.clf.predict_proba(X_test)[:, 1] for m in fitted])\n",
    "        elif isinstance(fitted[0].clf, keras_model):\n",
    "            preds_test = np.array([m.clf.predict(X_test) for m in fitted])\n",
    "        \n",
    "        print(f\"Mean Train AUC: {np.mean(train_scores):.2f} (+/- {np.std(train_scores):.2f})\")\n",
    "        print(f\"Mean OOB AUC: {np.mean(oob_scores):.2f} (+/- {np.std(oob_scores):.2f})\")\n",
    "        print(\"\")\n",
    "        \n",
    "    else: preds_test = np.array([m.predict_proba(X_test)[:, 1] for m in fitted])\n",
    "    print(\"Performance on hold out set:\")\n",
    "    if return_res:\n",
    "        test_auc = eval.score(y_test, preds_test.mean(axis=0), return_res)\n",
    "        return pd.DataFrame({\"method\": method,\n",
    "                             \"mean train auc\" : np.mean(train_scores),\n",
    "                                 \"mean cv auc\" : np.mean(oob_scores),\n",
    "                             \"mean test auc\" : test_auc}, index=[0])\n",
    "    else:\n",
    "        eval.score(y_test, preds_test.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.550271Z",
     "start_time": "2019-01-04T17:41:34.062Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_keras, evaluator, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Language model + neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.552596Z",
     "start_time": "2019-01-04T17:41:34.068Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.datasets import URLs\n",
    "from fastai.text import TextList\n",
    "from fastai.basic_data import DatasetType\n",
    "from fastai.text.learner import text_classifier_learner\n",
    "from fastai.text.learner import language_model_learner\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.555693Z",
     "start_time": "2019-01-04T17:41:34.074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = Path.cwd() / \"data\" / \"shared\" / \"fastai\"\n",
    "os.makedirs(PATH / \"exp\", exist_ok=True)\n",
    "EXP_PATH = PATH / \"exp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Train and Evaluate the Language Model (Metric : Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.557257Z",
     "start_time": "2019-01-04T17:41:34.079Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(df=data, path=EXP_PATH, cols=\"text\")\n",
    "             .random_split_by_pct()\n",
    "             .label_for_lm()\n",
    "             .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.559277Z",
     "start_time": "2019-01-04T17:41:34.084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.560873Z",
     "start_time": "2019-01-04T17:41:34.089Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.562759Z",
     "start_time": "2019-01-04T17:41:34.094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, 5e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.565514Z",
     "start_time": "2019-01-04T17:41:34.099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"lm_fit_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.567293Z",
     "start_time": "2019-01-04T17:41:34.106Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"lm_fit_head\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.569270Z",
     "start_time": "2019-01-04T17:41:34.111Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, 5e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.571018Z",
     "start_time": "2019-01-04T17:41:34.116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"lm_fine_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.572991Z",
     "start_time": "2019-01-04T17:41:34.121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save_encoder(\"lm_fine_tuned_enc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Train and Evaluate the Classifier (Metric = ROC AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.575844Z",
     "start_time": "2019-01-04T17:41:34.127Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i_train, i_valid = next(sss.split(data[\"text\"], data[\"interesting\"]))\n",
    "clf_df = pd.DataFrame({\"text\" : data[\"text\"], \"label\" : data[\"interesting\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.578359Z",
     "start_time": "2019-01-04T17:41:34.133Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_clf = (TextList.from_df(df=clf_df, path=EXP_PATH, cols=[\"text\"], vocab=data_lm.vocab)\n",
    "               .split_by_idx(i_valid)\n",
    "               .label_from_df(cols=\"label\")\n",
    "               .databunch(bs=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.580865Z",
     "start_time": "2019-01-04T17:41:34.141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clf, drop_mult=0.3)\n",
    "learn.load_encoder(\"lm_fine_tuned_enc\")\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.582470Z",
     "start_time": "2019-01-04T17:41:34.146Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.584463Z",
     "start_time": "2019-01-04T17:41:34.152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 5e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.586445Z",
     "start_time": "2019-01-04T17:41:34.159Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_first_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.588692Z",
     "start_time": "2019-01-04T17:41:34.165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_first_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.590833Z",
     "start_time": "2019-01-04T17:41:34.173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.592505Z",
     "start_time": "2019-01-04T17:41:34.177Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_second_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.594106Z",
     "start_time": "2019-01-04T17:41:34.184Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_second_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.595972Z",
     "start_time": "2019-01-04T17:41:34.189Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.598241Z",
     "start_time": "2019-01-04T17:41:34.196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_third_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.600905Z",
     "start_time": "2019-01-04T17:41:34.202Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_third_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.603695Z",
     "start_time": "2019-01-04T17:41:34.208Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(5e-4/(2.6**4),5e-4), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.606682Z",
     "start_time": "2019-01-04T17:41:34.214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p1_valid = learn.get_preds()[0][:, 1].numpy()\n",
    "p1_train = learn.get_preds(ds_type=DatasetType.Train)[0][:, 1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.608745Z",
     "start_time": "2019-01-04T17:41:34.219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y[i_train], p1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.611185Z",
     "start_time": "2019-01-04T17:41:34.225Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y[i_valid], p1_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:medium-classifier]",
   "language": "python",
   "name": "conda-env-medium-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
