{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General steps:\n",
    "\n",
    "- Split into train and test set (random and stratified, using sklearn train_test_split)\n",
    "- Learn transformations and word -> token mapping on train set \n",
    "- Train model on train set (potentially train severals model on different subsets and choose the best one / ensemble their predictions, in the latter case calculate out of bag score as well)\n",
    "- Calculate performance on train set (custom performance metric - choose threshold to get precision to >95% and evaluate recall based on that)\n",
    "- Apply transformations on test set, make predictions and evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:28:58.241589Z",
     "start_time": "2018-10-26T21:28:54.253456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# debugging\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# file system navigation\n",
    "from pathlib import Path\n",
    "\n",
    "# data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ml algorithms and evaluation metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "# sklearn specific stuff\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.util import decaying\n",
    "\n",
    "# misc\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Custom functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:51:06.862767Z",
     "start_time": "2018-10-26T21:51:06.821682Z"
    },
    "code_folding": [
     8,
     29,
     46
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomEvaluator():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_precision:float=0.95, pos_label:int=1):\n",
    "        self.target_precision = target_precision\n",
    "        self.pos_label = pos_label\n",
    "    \n",
    "    def print_npos(self, y_true:ndarray):\n",
    "        if self.pos_label not in np.unique(y_true):\n",
    "            warnings.warn(f\"Label '{self.pos_label}' is not in test set.\")\n",
    "        else:\n",
    "            npos = np.sum(y_true == self.pos_label)\n",
    "            print(f\"N POSITIVE CLASS: {npos} ({npos / len(y_true)*100:.2f}%)\")\n",
    "        \n",
    "    def score(self, y_true:ndarray, probas_pred:ndarray, display_results:bool=True):\n",
    "        #self.print_npos(y_true) # Log number of positive examples in the current test set\n",
    "        prs, rcs, ths = metrics.precision_recall_curve(y_true, probas_pred, pos_label=self.pos_label)\n",
    "    \n",
    "        if display_results:\n",
    "            auc = metrics.roc_auc_score(y_true, probas_pred)\n",
    "            print(f\"AUC SCORE: {auc:.2f}\")\n",
    "            results = pd.DataFrame({\"precision\": prs[:-1], \"recall\": rcs[:-1], \"threshold\": ths})\\\n",
    "                        .sort_values(by=[\"precision\", \"recall\"], ascending=[False, False])\n",
    "            if np.max(results.precision) > self.target_precision:\n",
    "                print(results[results.precision >= self.target_precision])\n",
    "            else:\n",
    "                print(results.head(3))\n",
    "\n",
    "    def score_extensive(self, y_true:ndarray, probas_pred:ndarray, display_results:bool=False):\n",
    "        self.print_npos(y_true) # Log number of positive examples in the current test set\n",
    "        prs, rcs, ths = metrics.precision_recall_curve(y_true, probas_pred, pos_label=self.pos_label)\n",
    "        \n",
    "        tmp_min_th = np.min(np.append(ths, 1.)[np.where(prs >= self.target_precision)])\n",
    "        min_th = tmp_min_th if tmp_min_th < 1. else np.min(ths)\n",
    "        res_pr = np.max(prs[np.where(ths == min_th)])\n",
    "        res_rc = np.max(rcs[np.where(ths == min_th)])\n",
    "        auc = metrics.roc_auc_score(y_true, probas_pred)\n",
    "        \n",
    "        self.print_results(min_th, res_pr, res_rc, auc) # Log resulting threshold, precision, recall and auc\n",
    "\n",
    "        if display_results:\n",
    "            display(pd.DataFrame({\"Precision\": prs[:-1], \"Recall\": rcs[:-1], \"Threshold\": ths})\\\n",
    "                    .sort_values(by=\"Precision\", ascending=False))\n",
    "            \n",
    "                \n",
    "    def print_results(self, min_th, res_pr, res_rc, auc):\n",
    "        print(f\"MINIMAL THRESHOLD: {min_th:.2f}\")\n",
    "        print(f\"RESULTING PRECISION: {res_pr:.2f} (Target: {self.target_precision})\")\n",
    "        print(f\"RESULTING RECALL: {res_rc:.2f}\")\n",
    "        print(f\"RESULTING AUC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:05.015010Z",
     "start_time": "2018-10-26T21:29:04.987704Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ScoredClf = namedtuple(\"ScoredClf\", [\n",
    "    \"clf\",\n",
    "    \"train_auc\",\n",
    "    \"oob_auc\"\n",
    "])\n",
    "\n",
    "def fit_ensemble(m, s:StratifiedShuffleSplit, X:ndarray, y:ndarray, **kwargs):\n",
    "    fitted_clfs = []\n",
    "\n",
    "    for i, split in enumerate(s.split(X, y)):\n",
    "        i_train = split[0]\n",
    "        i_test = split[1]\n",
    "        \n",
    "        print(\"#######################################\")\n",
    "        print(\"Training model number  \", i+1)\n",
    "        print(\"#######################################\")\n",
    "        print(\"\")\n",
    "\n",
    "        m.fit(X[i_train], y[i_train], **kwargs)\n",
    "        fitted_clf = copy.deepcopy(m)\n",
    "        \n",
    "        p1_train = fitted_clf.predict_proba(X[i_train])[:, 1]\n",
    "        p1_oob = fitted_clf.predict_proba(X[i_test])[:, 1]\n",
    "        \n",
    "        train_auc = metrics.roc_auc_score(y[i_train], p1_train)\n",
    "        oob_auc = metrics.roc_auc_score(y[i_test], p1_oob)\n",
    "        fitted_clfs.append(ScoredClf(fitted_clf, train_auc, oob_auc))\n",
    "        \n",
    "        #print(\"#####################\")\n",
    "        print(\"PERFORMANCE ON TRAIN\")\n",
    "        #print(\"#####################\")\n",
    "        print(\"\")\n",
    "        evaluator.score(y[i_train], p1_train)\n",
    "        \n",
    "        print(\"\")\n",
    "        #print(\"#####################\")\n",
    "        print(\"OOB PERFORMANCE\")\n",
    "        #print(\"#####################\")\n",
    "        print(\"\")\n",
    "        evaluator.score(y[i_test], p1_oob)\n",
    "        \n",
    "        print(\"\")\n",
    "    \n",
    "    return fitted_clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:06.248765Z",
     "start_time": "2018-10-26T21:29:06.214400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_ensemble(fitted:list, eval:CustomEvaluator, X:ndarray):\n",
    "    train_scores = [m.train_auc for m in fitted]\n",
    "    oob_scores = [m.oob_auc for m in fitted]\n",
    "    preds_test = np.array([m.clf.predict_proba(X)[:, 1] for m in fitted])\n",
    "\n",
    "    print(f\"Mean Train AUC: {np.mean(train_scores):.2f} (+/- {np.std(train_scores):.2f})\")\n",
    "    print(f\"Mean OOB AUC: {np.mean(oob_scores):.2f} (+/- {np.std(oob_scores):.2f})\")\n",
    "    print(\"\")\n",
    "    print(\"Performance on hold out set:\")\n",
    "    eval.score(y_test, preds_test.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_cv_scores(model, X, y, n_cv=10):\n",
    "    precision_scores = cross_val_score(model, X, y, cv=n_cv, scoring=\"precision\")\n",
    "    recall_scores = cross_val_score(model, X, y, cv=n_cv, scoring=\"recall\")\n",
    "    print(f\"Average precision score for {n_cv} splits: {precision_scores.mean():.2f} (+/- {precision_scores.std():.2f})\")\n",
    "    print(f\"Average recall score for {n_cv} splits: {recall_scores.mean():.2f} (+/- {recall_scores.std() * 2:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T16:23:53.465807Z",
     "start_time": "2018-10-21T16:23:53.459929Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_cv_score_auc(model, X, y, n_cv=10):\n",
    "    auc_scores = cross_val_score(model, X, y, cv=n_cv, scoring=\"roc_auc\")\n",
    "    print(f\"Average auc score for {n_cv} splits: {auc_scores.mean():.2f} (+/- {auc_scores.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T16:13:42.909551Z",
     "start_time": "2018-10-23T16:13:42.903097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_cv_auc(model, X, y, n_cv=10):\n",
    "    auc_cv = cross_validate(model, X, y,\n",
    "                                scoring=\"roc_auc\",\n",
    "                                cv=n_cv,\n",
    "                                n_jobs=-1,\n",
    "                                return_train_score=False,\n",
    "                                return_estimator=True)\n",
    "    auc_scores = auc_cv[\"test_score\"]\n",
    "    mean_auc = auc_scores.mean()\n",
    "    representative_estimator = auc_cv[\"estimator\"][np.argmin([np.abs(score - mean_auc) for score in auc_scores])]\n",
    "    print(f\"Average auc score for {n_cv} splits: {mean_auc:.2f} (+/- {auc_scores.std():.2f})\")\n",
    "    \n",
    "    return representative_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T16:49:35.059330Z",
     "start_time": "2018-10-21T16:49:35.049853Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(model, X, y):\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y, model.predict_proba(X)[:, 1])\n",
    "\n",
    "    step_kwargs = {\"step\": \"post\"}\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where=\"post\")\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\"Precision-Recall curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load preprocessed data and make split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:13.462532Z",
     "start_time": "2018-10-26T21:29:13.207948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(Path.cwd() / \"data\" / \"processed\" / \"train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:13.684028Z",
     "start_time": "2018-10-26T21:29:13.629092Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:15.987372Z",
     "start_time": "2018-10-26T21:29:15.951945Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = data[[\"claps\", \"reading_time\", \"text\"]]\n",
    "y = np.array(data[\"interesting\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3, random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:51:11.385834Z",
     "start_time": "2018-10-26T21:51:11.371693Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator = CustomEvaluator(target_precision=0.8)\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=6, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T16:13:16.555610Z",
     "start_time": "2018-10-23T16:13:16.549837Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_base = data[[\"claps\", \"reading_time\", \"interesting\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Visualize data distribution for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_index = 0\n",
    "y_index = 1\n",
    "target_names = [\"not interesting\", \"interesting\"]\n",
    "\n",
    "colors = [\"red\", \"green\"]\n",
    "\n",
    "for label, color in zip(range(len(data_base[\"interesting\"])), colors):\n",
    "    plt.scatter(np.array(data_base[data_base[\"interesting\"]==label].iloc[:, x_index]), \n",
    "                np.array(data_base[data_base[\"interesting\"]==label].iloc[:, y_index]),\n",
    "                label=target_names[label],\n",
    "                c=color)\n",
    "\n",
    "plt.xlabel(data_base.columns[x_index])\n",
    "plt.ylabel(data_base.columns[y_index])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.savefig(os.path.join(wd, \"output\", \"base_classifier.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Baseline classficiation model using author, claps and reading time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:48:39.703145Z",
     "start_time": "2018-10-26T21:48:39.683201Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_cols = [\"claps\", \"reading_time\"]\n",
    "X_train_num, X_test_num = np.array(X_train[num_cols]), np.array(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:48:39.907141Z",
     "start_time": "2018-10-26T21:48:39.899842Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(min_samples_leaf=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:48:43.029345Z",
     "start_time": "2018-10-26T21:48:42.545697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted = fit_ensemble(rf, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:51:14.273201Z",
     "start_time": "2018-10-26T21:51:14.170745Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted, evaluator, X_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T16:13:56.804070Z",
     "start_time": "2018-10-23T16:13:53.798814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_model = get_cv_auc(RandomForestClassifier(), X_num, y, n_cv=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T16:21:28.607292Z",
     "start_time": "2018-10-23T16:21:28.601934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s = model_selection.StratifiedShuffleSplit(n_splits=4, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T16:32:32.143735Z",
     "start_time": "2018-10-23T16:32:32.073254Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i, split in enumerate(s.split(X_num, y_num)):\n",
    "    i_train = split[0]\n",
    "    i_test = split[1]\n",
    "    print(\"Training model number  \", i)\n",
    "    rf_model.fit(X_num.iloc[i_train, :], y_num[i_train])\n",
    "    print(\"AUC on the test set:\")\n",
    "    print(metrics.roc_auc_score(y_num[i_test], rf_model.predict_proba(X_num.iloc[i_test, :])[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Text based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:25.516507Z",
     "start_time": "2018-10-26T21:29:25.503010Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "X_train_text, X_test_text = np.array(X_train[text_col]), np.array(X_test[text_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:45:20.557778Z",
     "start_time": "2018-10-24T12:45:20.552611Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = data[\"text\"]\n",
    "y = data[\"interesting\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create true hold out set to simulate future articles coming in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:20:47.213764Z",
     "start_time": "2018-10-24T13:20:47.208360Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_70 = X_text[0:70]\n",
    "y_70 = y[0:70]\n",
    "\n",
    "X_100 = X_text[70:]\n",
    "y_100 = y[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:24:47.508398Z",
     "start_time": "2018-10-24T13:24:47.496691Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_70 = X_70.reset_index().drop(\"index\", axis=1)[\"text\"]\n",
    "X_100 = X_100.reset_index().drop(\"index\", axis=1)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:25:34.020042Z",
     "start_time": "2018-10-24T13:25:34.009723Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_70 = y_70.reset_index().drop(\"index\", axis=1)[\"interesting\"]\n",
    "y_100 = y_100.reset_index().drop(\"index\", axis=1)[\"interesting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T17:16:28.204341Z",
     "start_time": "2018-10-23T17:16:28.198673Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_text_train, X_text_test, y_train, y_test = train_test_split(X_text,\n",
    "                                                              y,\n",
    "                                                              test_size=0.3,\n",
    "                                                              random_state=42,\n",
    "                                                              stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### CountVectorizer + random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:51:49.619214Z",
     "start_time": "2018-10-26T21:51:49.604731Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:51:50.506832Z",
     "start_time": "2018-10-26T21:51:50.492668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec = make_pipeline(count_vectorizer, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:52:55.731879Z",
     "start_time": "2018-10-26T21:52:51.394690Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_countvec = fit_ensemble(pipe_countvec, sss, X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:53:40.245510Z",
     "start_time": "2018-10-26T21:53:39.281419Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_countvec, evaluator, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:10:58.877105Z",
     "start_time": "2018-10-21T17:10:56.454940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator = get_cv_auc(pipe, X_text_train, y_train, n_cv=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:10:59.048427Z",
     "start_time": "2018-10-21T17:10:58.880179Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(estimator, X_text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Some optimization, i.e. preprocessing and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:09:29.267522Z",
     "start_time": "2018-10-21T17:09:29.256534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer()),\n",
    "    (\"rf\", RandomForestClassifier())\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=9000)}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=8,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          cv=10,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:09:50.021782Z",
     "start_time": "2018-10-21T17:09:32.004989Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = grid.fit(X_text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:09:50.032580Z",
     "start_time": "2018-10-21T17:09:50.025314Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_auc = grid.cv_results_[\"mean_test_score\"].mean()\n",
    "std_auc = grid.cv_results_[\"std_test_score\"].mean()\n",
    "print(f\"Average auc score: {avg_auc:.2f} (+/- {std_auc:.2f})\")\n",
    "# pd.DataFrame.from_dict(grid.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:09:50.477014Z",
     "start_time": "2018-10-21T17:09:50.036911Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(grid.best_estimator_, X_text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### TfidfVectorizer + random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:01:20.149332Z",
     "start_time": "2018-10-26T22:01:20.134098Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_model = RandomForestClassifier(min_samples_leaf=5, n_jobs=-1, n_estimators=10, max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:01:20.556970Z",
     "start_time": "2018-10-26T22:01:20.543232Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf = make_pipeline(tfidf_vectorizer, tfidf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:01:28.303060Z",
     "start_time": "2018-10-26T22:01:20.926331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_tfidf = fit_ensemble(pipe_tfidf, sss, X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:01:30.151438Z",
     "start_time": "2018-10-26T22:01:28.310924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_tfidf, evaluator, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:11:44.554118Z",
     "start_time": "2018-10-21T17:11:42.077385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator = get_cv_auc(pipe, X_text_train, y_train, n_cv=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:11:44.713209Z",
     "start_time": "2018-10-21T17:11:44.557498Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(estimator, X_text_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:24:03.112478Z",
     "start_time": "2018-10-21T17:23:56.079623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 3), max_df=0.8, min_df=0.2, max_features=5000)\n",
    "\n",
    "pipe = make_pipeline(vectorizer, RandomForestClassifier())\n",
    "\n",
    "estimator = get_cv_auc(pipe, X_text_train, y_train, n_cv=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:24:05.661623Z",
     "start_time": "2018-10-21T17:24:05.156724Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(estimator, X_text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:20:25.557203Z",
     "start_time": "2018-10-21T17:20:25.541996Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", TfidfVectorizer()),\n",
    "    (\"rf\", RandomForestClassifier())\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=9000)}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=8,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          cv=10,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:20:50.325253Z",
     "start_time": "2018-10-21T17:20:28.199477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = grid.fit(X_text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:20:50.337135Z",
     "start_time": "2018-10-21T17:20:50.330807Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_auc = grid.cv_results_[\"mean_test_score\"].mean()\n",
    "std_auc = grid.cv_results_[\"std_test_score\"].mean()\n",
    "print(f\"Average auc score: {avg_auc:.2f} (+/- {std_auc:.2f})\")\n",
    "# pd.DataFrame.from_dict(grid.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:20:50.752909Z",
     "start_time": "2018-10-21T17:20:50.349418Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(grid.best_estimator_, X_text_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Advanced tokenization and lemmatization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:24:12.630133Z",
     "start_time": "2018-10-24T15:24:11.163316Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:24:32.158176Z",
     "start_time": "2018-10-24T15:24:31.110958Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(X_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:32:02.077992Z",
     "start_time": "2018-10-24T15:31:59.682984Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T14:52:53.664047Z",
     "start_time": "2018-10-24T14:52:53.144816Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### SpaCy language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Instructions from SpaCy documentation](https://spacy.io/usage/training#section-textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:41.353281Z",
     "start_time": "2018-10-26T21:29:41.315988Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomSpacyClassifier():\n",
    "    \"\"\" Wrapper for spaCy's text classification that enables integration with sklearn.metrics.cross_validate\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._estimator_type = \"classifier\"\n",
    "        \n",
    "        self.nlp = None\n",
    "        self.label = None\n",
    "        self.train_data = None\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return dict()\n",
    "    \n",
    "    def add_textcat(self, label):\n",
    "        self.label = label\n",
    "        if \"textcat\" not in self.nlp.pipe_names:\n",
    "            textcat = self.nlp.create_pipe(\"textcat\")\n",
    "            self.nlp.add_pipe(textcat, last=True)\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        else:\n",
    "            textcat = self.nlp.get_pipe(\"textcat\")\n",
    "        textcat.add_label(label)\n",
    "    \n",
    "    def fit(self, X, y, n_iter=10, **kwargs):\n",
    "        \n",
    "        self.nlp = spacy.load(\"en\")\n",
    "        self.add_textcat(\"interesting\")\n",
    "        self.train_data = [(e, {\"cats\": {self.label: bool(l)}}) for e, l in zip(X, y)]\n",
    "        \n",
    "        drop_rate = kwargs[\"drop_rate\"]\n",
    "        \n",
    "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != \"textcat\"]\n",
    "        with self.nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "            optimizer = self.nlp.begin_training()\n",
    "            for i in range(n_iter):\n",
    "                print(f\"EPOCH {i+1}\")\n",
    "                losses = {}\n",
    "                batches = minibatch(self.train_data, size=compounding(4., 16., 1.001))\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    self.nlp.update(texts, annotations, sgd=optimizer, drop=drop_rate,\n",
    "                               losses=losses)\n",
    "                loss = losses[\"textcat\"]\n",
    "                print(f\"LOSS: {loss}\")\n",
    "                print(\"\")\n",
    "                \n",
    "    def predict_proba(self, X):\n",
    "        p1_scores = [np.float64(self.nlp(sample_text).cats[\"interesting\"]) for sample_text in X]\n",
    "        \n",
    "        return np.array([[1. - score, score] for score in p1_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:29:48.940076Z",
     "start_time": "2018-10-26T21:29:48.922701Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf_spacy = CustomSpacyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:43:32.281501Z",
     "start_time": "2018-10-26T21:30:13.710600Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_spacy = fit_ensemble(clf_spacy, sss, X_train_text, y_train, n_iter=5, drop_rate=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T21:47:00.185838Z",
     "start_time": "2018-10-26T21:43:32.359553Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_spacy, evaluator, X_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train several custom classifiers and evaluate their performance on the true hold out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:29:11.842938Z",
     "start_time": "2018-10-24T13:26:07.503373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_clfs = []\n",
    "\n",
    "for i, split in enumerate(s.split(X_70, y_70)):\n",
    "    i_train = split[0]\n",
    "    i_test = split[1]\n",
    "    \n",
    "    print(\"Training model number  \", i)\n",
    "    print(\"\")\n",
    "    print(\"Training IDs: \", i_train)\n",
    "    print(\"Test IDs: \", i_test)\n",
    "    \n",
    "    clf.fit(X_70[i_train], y_70[i_train], n_iter=5, drop_rate=0.4)\n",
    "    fitted_clf = copy.deepcopy(clf)\n",
    "    test_auc = metrics.roc_auc_score(y_70[i_test], clf.predict_proba(X_70[i_test])[:, 1])\n",
    "    fitted_clfs.append((fitted_clf, test_auc))\n",
    "\n",
    "    print(\"AUC on the test set: \", test_auc)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:29:29.229181Z",
     "start_time": "2018-10-24T13:29:29.222974Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores = [score for _, score in fitted_clfs]\n",
    "print(\"Mean AUC: \", np.mean(scores))\n",
    "print(\"Std deviation of AUC: \", np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:31:33.242453Z",
     "start_time": "2018-10-24T13:29:31.223222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = np.array([c.predict_proba(X_100)[:, 1] for c, _ in fitted_clfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:31:33.257840Z",
     "start_time": "2018-10-24T13:31:33.245011Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"AUC on the hold out set: \", metrics.roc_auc_score(y_100, preds.mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T19:09:41.728665Z",
     "start_time": "2018-10-21T19:08:15.508664Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_cv = cross_validate(clf, X_text_train, y_train,\n",
    "                            scoring=\"roc_auc\",\n",
    "                            cv=2,\n",
    "                            n_jobs=1,\n",
    "                            return_train_score=False,\n",
    "                            return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T19:09:48.231686Z",
     "start_time": "2018-10-21T19:09:48.222714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "auc_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:25:28.110965Z",
     "start_time": "2018-10-21T17:25:27.194211Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:25:32.274827Z",
     "start_time": "2018-10-21T17:25:32.269573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\"textcat\")\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:25:34.572100Z",
     "start_time": "2018-10-21T17:25:34.562167Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textcat.add_label(\"interesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:25:39.933733Z",
     "start_time": "2018-10-21T17:25:39.926488Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = [(example, {\"cats\": {\"interesting\": bool(label)}}) for example, label in zip(X_text_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:26:16.087658Z",
     "start_time": "2018-10-21T17:26:16.080752Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dropout = decaying(0.6, 0.2, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 20:\n",
    "    print(next(dropout))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "size=compounding(4., 16., 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 20:\n",
    "    print(next(size))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_iter):\n",
    "        print(f\"EPOCH {i+1}\")\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 16., 1.5))\n",
    "        print(len(next(batches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:27:08.202431Z",
     "start_time": "2018-10-21T17:26:17.696164Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(n_iter):\n",
    "        print(f\"EPOCH {i+1}\")\n",
    "        losses = {}\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 16., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.3,\n",
    "                       losses=losses)\n",
    "        loss = losses[\"textcat\"]\n",
    "        print(f\"LOSS: {loss}\")\n",
    "        print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:27:41.161981Z",
     "start_time": "2018-10-21T17:27:28.334154Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_scores = [nlp(sample_text).cats[\"interesting\"] for sample_text in X_text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T18:36:38.544596Z",
     "start_time": "2018-10-21T18:36:37.938629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "te = [np.float64(nlp(sample_text).cats[\"interesting\"]) for sample_text in X_text_test[0:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T18:34:36.001249Z",
     "start_time": "2018-10-21T18:34:35.997389Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "te2 = np.array([[1. - score, score] for score in te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T18:34:36.546165Z",
     "start_time": "2018-10-21T18:34:36.539196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "te2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T18:36:52.812941Z",
     "start_time": "2018-10-21T18:36:52.802279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.float64(te[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:29:17.285373Z",
     "start_time": "2018-10-21T17:29:17.277144Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:30:21.381422Z",
     "start_time": "2018-10-21T17:30:21.260650Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision, recall, _ = metrics.precision_recall_curve(y_test, test_scores)\n",
    "\n",
    "step_kwargs = {\"step\": \"post\"}\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where=\"post\")\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\"Precision-Recall curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:21:22.376884Z",
     "start_time": "2018-10-26T22:21:22.236695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(Path.cwd() / \"data\" / \"shared\" / \"train_data.parquet\")\n",
    "\n",
    "X = data[[\"claps\", \"reading_time\", \"text\"]]\n",
    "y = np.array(data[\"interesting\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3, random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:21:33.670266Z",
     "start_time": "2018-10-26T22:21:33.647353Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_fastai = pd.DataFrame({\"label\": y_train, \"text\": X_train[\"text\"]})\n",
    "test_fastai = pd.DataFrame({\"label\": y_test, \"text\": X_test[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:25:40.878762Z",
     "start_time": "2018-10-26T22:25:40.864858Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = Path.cwd() / \"data\" / \"shared\" / \"fastai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T22:25:47.131761Z",
     "start_time": "2018-10-26T22:25:47.064549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_fastai.to_csv(path / \"train.csv\", sep=\",\", index=False, header=False)\n",
    "train_fastai.to_csv(path / \"test.csv\", sep=\",\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator = CustomEvaluator(target_precision=0.8)\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=6, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing valid.\n"
     ]
    }
   ],
   "source": [
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_csv(path)\n",
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_csv(path, vocab=data_lm.train_ds.vocab, bs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Download pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This did not work, had to do this using curl from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# URLs.download_wt103_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Train language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:26\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      5.129876    4.502655    0.242058  (00:06)\n",
      "2      4.669750    3.958226    0.271622  (00:06)\n",
      "3      4.352579    3.780446    0.286204  (00:06)\n",
      "4      4.158467    3.748044    0.289051  (00:06)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn = RNNLearner.language_model(data_lm, pretrained_fnames=['lstm_wt103', 'itos_wt103'], drop_mult=0.5)\n",
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unfreeze and fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:31\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      3.991501    3.651473    0.300154  (00:07)\n",
      "2      3.908763    3.523459    0.316158  (00:07)\n",
      "3      3.810472    3.444433    0.324955  (00:07)\n",
      "4      3.737535    3.413374    0.328164  (00:07)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Save encoder to use for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 01:50\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      0.724800    0.636203    0.692308  (00:27)\n",
      "2      0.635331    0.501309    0.780220  (00:27)\n",
      "3      0.565197    0.541426    0.703297  (00:27)\n",
      "4      0.546613    0.515166    0.703297  (00:28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn = RNNLearner.classifier(data_clas, drop_mult=0.5)\n",
    "learn.load_encoder('ft_enc')\n",
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 01:52\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      0.534426    0.456059    0.802198  (00:29)\n",
      "2      0.502023    0.393139    0.857143  (00:28)\n",
      "3      0.453869    0.436747    0.846154  (00:27)\n",
      "4      0.410717    0.399213    0.857143  (00:27)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(4, slice(5e-3/2., 5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 02:34\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      0.426951    0.362562    0.879121  (00:39)\n",
      "2      0.393601    0.347144    0.901099  (00:37)\n",
      "3      0.372209    0.348093    0.868132  (00:40)\n",
      "4      0.360141    0.329950    0.879121  (00:37)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(2e-3/100, 2e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ActivationStats',\n",
       " 'AdamW',\n",
       " 'AdaptiveConcatPool2d',\n",
       " 'AffineFunc',\n",
       " 'AffineMatrix',\n",
       " 'AnnealFunc',\n",
       " 'Any',\n",
       " 'AnyStr',\n",
       " 'ArgStar',\n",
       " 'AverageMetric',\n",
       " 'BOS',\n",
       " 'BaseTextDataset',\n",
       " 'BaseTokenizer',\n",
       " 'BatchSampler',\n",
       " 'BatchSamples',\n",
       " 'BnFreeze',\n",
       " 'BoolOrTensor',\n",
       " 'ByteTensor',\n",
       " 'Callable',\n",
       " 'Callback',\n",
       " 'CallbackHandler',\n",
       " 'CallbackList',\n",
       " 'Classes',\n",
       " 'Collection',\n",
       " 'Counter',\n",
       " 'CrossEntropyFlat',\n",
       " 'DataBunch',\n",
       " 'DataFrame',\n",
       " 'DataFrameOrChunks',\n",
       " 'DataLoader',\n",
       " 'Dataset',\n",
       " 'DatasetBase',\n",
       " 'Debugger',\n",
       " 'DeviceDataLoader',\n",
       " 'Dict',\n",
       " 'DoubleTensor',\n",
       " 'EarlyStoppingCallback',\n",
       " 'EmbeddingDropout',\n",
       " 'Enum',\n",
       " 'F',\n",
       " 'FLD',\n",
       " 'Fbeta',\n",
       " 'FilePathList',\n",
       " 'Flatten',\n",
       " 'FloatOrTensor',\n",
       " 'FloatTensor',\n",
       " 'Floats',\n",
       " 'GeneralScheduler',\n",
       " 'GradientClipping',\n",
       " 'HalfTensor',\n",
       " 'Hashable',\n",
       " 'Hook',\n",
       " 'HookCallback',\n",
       " 'HookFunc',\n",
       " 'Hooks',\n",
       " 'ImgLabel',\n",
       " 'ImgLabels',\n",
       " 'In',\n",
       " 'IntEnum',\n",
       " 'IntOrTensor',\n",
       " 'ItemBase',\n",
       " 'ItemsList',\n",
       " 'Iterable',\n",
       " 'Iterator',\n",
       " 'KWArgs',\n",
       " 'KeyFunc',\n",
       " 'LRFinder',\n",
       " 'LabelDataset',\n",
       " 'Lambda',\n",
       " 'LambdaFunc',\n",
       " 'LanguageModelLoader',\n",
       " 'LayerFunc',\n",
       " 'Learner',\n",
       " 'LearnerCallback',\n",
       " 'LightingFunc',\n",
       " 'LinearDecoder',\n",
       " 'List',\n",
       " 'ListOrItem',\n",
       " 'ListRules',\n",
       " 'ListSizes',\n",
       " 'LogitTensorImage',\n",
       " 'LongTensor',\n",
       " 'LossFunction',\n",
       " 'MSELossFlat',\n",
       " 'Mapping',\n",
       " 'MasterBar',\n",
       " 'MetricFunc',\n",
       " 'MetricFuncList',\n",
       " 'MetricsList',\n",
       " 'MixUpCallback',\n",
       " 'MixUpLoss',\n",
       " 'MixedPrecision',\n",
       " 'Model',\n",
       " 'ModuleList',\n",
       " 'MultiBatchRNNCore',\n",
       " 'NPArrayList',\n",
       " 'NPArrayMask',\n",
       " 'NPArrayableList',\n",
       " 'NPImage',\n",
       " 'NewType',\n",
       " 'Number',\n",
       " 'ORTH',\n",
       " 'OneCycleScheduler',\n",
       " 'OptDataFrame',\n",
       " 'OptListOrItem',\n",
       " 'OptLossFunc',\n",
       " 'OptMetrics',\n",
       " 'OptOptimizer',\n",
       " 'OptRange',\n",
       " 'OptSplitFunc',\n",
       " 'OptStats',\n",
       " 'OptStrList',\n",
       " 'OptStrTuple',\n",
       " 'OptimWrapper',\n",
       " 'Optional',\n",
       " 'OrderedDict',\n",
       " 'Out',\n",
       " 'PAD',\n",
       " 'PBar',\n",
       " 'ParamList',\n",
       " 'Patch',\n",
       " 'Path',\n",
       " 'PathOrStr',\n",
       " 'PixelFunc',\n",
       " 'Point',\n",
       " 'Points',\n",
       " 'PoolFlatten',\n",
       " 'PoolingLinearClassifier',\n",
       " 'ProcessPoolExecutor',\n",
       " 'ProgressBar',\n",
       " 'RNNCore',\n",
       " 'RNNDropout',\n",
       " 'RNNLearner',\n",
       " 'RNNTrainer',\n",
       " 'Rank0Tensor',\n",
       " 'Recorder',\n",
       " 'ReduceLROnPlateauCallback',\n",
       " 'ResizeBatch',\n",
       " 'Sampler',\n",
       " 'SaveModelCallback',\n",
       " 'Sequence',\n",
       " 'SequentialRNN',\n",
       " 'Series',\n",
       " 'ShortTensor',\n",
       " 'ShowGraph',\n",
       " 'SimpleNamespace',\n",
       " 'Sizes',\n",
       " 'SmoothenValue',\n",
       " 'SortSampler',\n",
       " 'SortishSampler',\n",
       " 'SpacyTokenizer',\n",
       " 'SplitArrayList',\n",
       " 'SplitFunc',\n",
       " 'SplitFuncOrIdxList',\n",
       " 'StartOptEnd',\n",
       " 'StdUpsample',\n",
       " 'Stepper',\n",
       " 'StrList',\n",
       " 'TK_REP',\n",
       " 'TK_UP',\n",
       " 'TK_WREP',\n",
       " 'Tensor',\n",
       " 'TensorDataset',\n",
       " 'TensorImage',\n",
       " 'TensorImageSize',\n",
       " 'TensorOrNumList',\n",
       " 'TensorOrNumber',\n",
       " 'Tensors',\n",
       " 'TerminateOnNaNCallback',\n",
       " 'TextClasDataBunch',\n",
       " 'TextDataBunch',\n",
       " 'TextDataset',\n",
       " 'TextLMDataBunch',\n",
       " 'TextMtd',\n",
       " 'ThreadPoolExecutor',\n",
       " 'Tokenizer',\n",
       " 'Tokens',\n",
       " 'TrackerCallback',\n",
       " 'TrainingPhase',\n",
       " 'Tuple',\n",
       " 'TypeVar',\n",
       " 'UNK',\n",
       " 'URLs',\n",
       " 'Union',\n",
       " 'Vocab',\n",
       " 'WeightDropout',\n",
       " 'Weights',\n",
       " '_',\n",
       " '_12',\n",
       " '_14',\n",
       " '_15',\n",
       " '_19',\n",
       " '_20',\n",
       " '_24',\n",
       " '_27',\n",
       " '_31',\n",
       " '_32',\n",
       " '_34',\n",
       " '_42',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i23',\n",
       " '_i24',\n",
       " '_i25',\n",
       " '_i26',\n",
       " '_i27',\n",
       " '_i28',\n",
       " '_i29',\n",
       " '_i3',\n",
       " '_i30',\n",
       " '_i31',\n",
       " '_i32',\n",
       " '_i33',\n",
       " '_i34',\n",
       " '_i35',\n",
       " '_i36',\n",
       " '_i37',\n",
       " '_i38',\n",
       " '_i39',\n",
       " '_i4',\n",
       " '_i40',\n",
       " '_i41',\n",
       " '_i42',\n",
       " '_i43',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'a',\n",
       " 'abc',\n",
       " 'abstractmethod',\n",
       " 'abstractproperty',\n",
       " 'accuracy',\n",
       " 'accuracy_thresh',\n",
       " 'annealing_cos',\n",
       " 'annealing_exp',\n",
       " 'annealing_linear',\n",
       " 'annealing_no',\n",
       " 'annealing_poly',\n",
       " 'apply_init',\n",
       " 'apply_leaf',\n",
       " 'arange_of',\n",
       " 'array',\n",
       " 'arrays_split',\n",
       " 'as_tensor',\n",
       " 'attrgetter',\n",
       " 'basic_data',\n",
       " 'basic_train',\n",
       " 'bn2float',\n",
       " 'bn_drop_lin',\n",
       " 'bn_types',\n",
       " 'calc_loss',\n",
       " 'callback',\n",
       " 'callbacks',\n",
       " 'camel2snake',\n",
       " 'children',\n",
       " 'collections',\n",
       " 'cond_init',\n",
       " 'conv2d',\n",
       " 'conv2d_relu',\n",
       " 'conv2d_trans',\n",
       " 'conv_layer',\n",
       " 'convert_weights',\n",
       " 'copy',\n",
       " 'core',\n",
       " 'cos',\n",
       " 'csv',\n",
       " 'data',\n",
       " 'data_clas',\n",
       " 'data_collate',\n",
       " 'data_lm',\n",
       " 'dataclass',\n",
       " 'datapath4file',\n",
       " 'datasets',\n",
       " 'deal_caps',\n",
       " 'deepcopy',\n",
       " 'default_lr',\n",
       " 'default_rules',\n",
       " 'default_spec_tok',\n",
       " 'default_wd',\n",
       " 'defaultdict',\n",
       " 'defaults',\n",
       " 'dice',\n",
       " 'do_annealing_poly',\n",
       " 'doc',\n",
       " 'download_data',\n",
       " 'download_url',\n",
       " 'dropout_mask',\n",
       " 'error_rate',\n",
       " 'even_mults',\n",
       " 'exit',\n",
       " 'exp',\n",
       " 'exp_rmspe',\n",
       " 'extract_kwargs',\n",
       " 'fastai_types',\n",
       " 'field',\n",
       " 'find_classes',\n",
       " 'first_layer',\n",
       " 'fit',\n",
       " 'fit_one_cycle',\n",
       " 'fix_html',\n",
       " 'flatten_model',\n",
       " 'fp16',\n",
       " 'functools',\n",
       " 'gc',\n",
       " 'gen_doc',\n",
       " 'general_sched',\n",
       " 'get_chunk_length',\n",
       " 'get_embedding',\n",
       " 'get_ipython',\n",
       " 'get_language_model',\n",
       " 'get_preds',\n",
       " 'get_rnn_classifier',\n",
       " 'get_total_length',\n",
       " 'gzip',\n",
       " 'hashlib',\n",
       " 'hook_output',\n",
       " 'hook_outputs',\n",
       " 'hooks',\n",
       " 'html',\n",
       " 'idx_dict',\n",
       " 'ifnone',\n",
       " 'imports',\n",
       " 'in_channels',\n",
       " 'inspect',\n",
       " 'is_listy',\n",
       " 'is_tuple',\n",
       " 'itemgetter',\n",
       " 'itertools',\n",
       " 'json',\n",
       " 'layers',\n",
       " 'learn',\n",
       " 'learner',\n",
       " 'listify',\n",
       " 'lm_split',\n",
       " 'log',\n",
       " 'loss_batch',\n",
       " 'lr_find',\n",
       " 'lr_finder',\n",
       " 'master_bar',\n",
       " 'math',\n",
       " 'maybe_copy',\n",
       " 'metrics',\n",
       " 'mimetypes',\n",
       " 'mixup',\n",
       " 'model2half',\n",
       " 'model_sizes',\n",
       " 'model_type',\n",
       " 'models',\n",
       " 'namedtuple',\n",
       " 'nn',\n",
       " 'noop',\n",
       " 'np',\n",
       " 'np2model_tensor',\n",
       " 'np_address',\n",
       " 'num_children',\n",
       " 'num_cpus',\n",
       " 'one_cycle',\n",
       " 'one_cycle_scheduler',\n",
       " 'operator',\n",
       " 'optim',\n",
       " 'os',\n",
       " 'pad_collate',\n",
       " 'partial',\n",
       " 'partition',\n",
       " 'partition_by_cores',\n",
       " 'patches',\n",
       " 'path',\n",
       " 'patheffects',\n",
       " 'pd',\n",
       " 'pickle',\n",
       " 'plt',\n",
       " 'progress_bar',\n",
       " 'quit',\n",
       " 'random',\n",
       " 'random_split',\n",
       " 'range_children',\n",
       " 'range_of',\n",
       " 're',\n",
       " 'read_classes',\n",
       " 'reduce',\n",
       " 'replace_rep',\n",
       " 'replace_wrep',\n",
       " 'requests',\n",
       " 'requires_grad',\n",
       " 'rm_useless_spaces',\n",
       " 'rnn',\n",
       " 'rnn_classifier_split',\n",
       " 'scipy',\n",
       " 'series2cat',\n",
       " 'set_bn_eval',\n",
       " 'set_trace',\n",
       " 'show_install',\n",
       " 'shutil',\n",
       " 'simple_cnn',\n",
       " 'sin',\n",
       " 'singledispatch',\n",
       " 'spacy',\n",
       " 'spec_add_spaces',\n",
       " 'split_bn_bias',\n",
       " 'split_model',\n",
       " 'split_model_idx',\n",
       " 'std_upsample_head',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tarfile',\n",
       " 'tensor',\n",
       " 'to_data',\n",
       " 'to_detach',\n",
       " 'to_device',\n",
       " 'to_fp16',\n",
       " 'to_half',\n",
       " 'to_int',\n",
       " 'to_np',\n",
       " 'torch',\n",
       " 'torch_core',\n",
       " 'tracker',\n",
       " 'train',\n",
       " 'train_epoch',\n",
       " 'trainable_params',\n",
       " 'trange_of',\n",
       " 'transform',\n",
       " 'trunc_normal_',\n",
       " 'typing',\n",
       " 'uniqueify',\n",
       " 'untar_data',\n",
       " 'url2name',\n",
       " 'utils',\n",
       " 'validate',\n",
       " 'version',\n",
       " 'warn',\n",
       " 'warnings',\n",
       " 'yaml']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:37:56.767829Z",
     "start_time": "2018-10-21T17:37:56.757286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomEvaluator():\n",
    "    \"\"\" Simple class holding data and functionality related to evaluating a classifier's performance\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, scores):\n",
    "        self.df = pd.DataFrame({\"text\": texts, \"label\": labels, \"score\": scores})\n",
    "        self.group_means = self.df.groupby(by=\"label\").mean()\n",
    "        \n",
    "    def get_scores(self, thresholds=[0.25, 0.5, 0.75]):\n",
    "        if isinstance(thresholds, float):\n",
    "            thresholds = [thresholds]\n",
    "        \n",
    "        tps = [1e-8]*len(thresholds)  # True positives\n",
    "        fps = [1e-8]*len(thresholds)  # False positives\n",
    "        fns = [1e-8]*len(thresholds)  # False negatives\n",
    "        tns = [1e-8]*len(thresholds)  # True negatives\n",
    "        \n",
    "        for i, t in enumerate(thresholds):\n",
    "            for truth, pred in zip(self.df[\"label\"], self.df[\"score\"] > t):\n",
    "                if truth and pred:\n",
    "                    tps[i] += 1.\n",
    "                elif not truth and pred:\n",
    "                    fps[i] += 1.\n",
    "                elif truth and not pred:\n",
    "                    fns[i] += 1.\n",
    "                elif not truth and not pred:\n",
    "                    tns[i] += 1.\n",
    "        \n",
    "        precisions = [tp / (tp + fp) for tp, fp in zip(tps, fps)]\n",
    "        recalls = [tp / (tp + fn) for tp, fn in zip(tps, fns)]\n",
    "        f_scores = [2 * (p * r) / (p + r) for p, r in zip(precisions, recalls)]\n",
    "        accuracies = [(tp + tn) / (tp + fp + fn + tn) for tp, fp, fn, tn in zip(tps, fps, fns, tns)]\n",
    "        \n",
    "        score_df = pd.DataFrame({\"threshold\": thresholds,\n",
    "                                 \"precision\": precisions,\n",
    "                                 \"recall\": recalls,\n",
    "                                 \"f_score\": f_scores,\n",
    "                                 \"accuracy\": accuracies})\n",
    "        \n",
    "        print(score_df)\n",
    "        self.score_df = score_df\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:38:06.430797Z",
     "start_time": "2018-10-21T17:38:06.417832Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_evaluator = CustomEvaluator(X_text_test, y_test, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:38:07.477180Z",
     "start_time": "2018-10-21T17:38:07.465921Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_evaluator.group_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T17:38:10.097825Z",
     "start_time": "2018-10-21T17:38:10.081271Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_evaluator.get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
