{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:32.596280Z",
     "start_time": "2018-12-27T12:18:25.506865Z"
    }
   },
   "outputs": [],
   "source": [
    "# file system navigation\n",
    "from pathlib import Path\n",
    "\n",
    "# data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ml algorithms and evaluation metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "# sklearn specifics\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.util import decaying\n",
    "\n",
    "# misc\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:34.585272Z",
     "start_time": "2018-12-27T12:18:34.579475Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path.cwd() / \"data\" / \"processed\"\n",
    "OUTPUT_PATH = Path.cwd() / \"reports\" / \"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Helper visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate a plot of interest in Machine Learning over time based on Google trends data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T17:29:36.517400Z",
     "start_time": "2018-11-25T17:29:36.477797Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml = pd.read_csv(Path.cwd() / \"data\" / \"trends-ml.csv\",\n",
    "                               skiprows=3,\n",
    "                               header=None,\n",
    "                               names=[\"date\", \"interest\"],\n",
    "                               parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T17:29:37.623420Z",
     "start_time": "2018-11-25T17:29:37.580269Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T17:49:01.287424Z",
     "start_time": "2018-11-25T17:49:00.879473Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interest_plot = google_trends_ml.plot(x=\"date\",\n",
    "                      y=\"interest\",\n",
    "                      legend=False)\n",
    "interest_plot.set_xlabel(\"Date\")\n",
    "interest_plot.set_ylabel(\"Relative interest\")\n",
    "interest_plot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T17:49:03.023682Z",
     "start_time": "2018-11-25T17:49:02.825958Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = interest_plot.get_figure()\n",
    "fig.savefig(Path.cwd() / \"reports\" / \"images\" / \"interest-in-ml.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preprocessed data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:40.070760Z",
     "start_time": "2018-12-27T12:18:39.823490Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(DATA_PATH / \"train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:40.385178Z",
     "start_time": "2018-12-27T12:18:40.374597Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data[[\"claps\", \"reading_time\", \"text\"]]\n",
    "y = np.array(data[\"interesting\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3, random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize custom evaluator and shuffle split generator to use across all modeling approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:42.915518Z",
     "start_time": "2018-12-27T12:18:42.910351Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = CustomEvaluator(target_precision=0.8)\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=6, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a vocabulary of words specific to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:45.636653Z",
     "start_time": "2018-12-27T12:18:45.598119Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_PATH = Path.cwd() / \"resources\" / \"embeddings\"\n",
    "top_10k = pd.read_table(EMBEDDING_PATH / \"google-10000-english\" / \"google-10000-english.txt\", header=None)\n",
    "top_10k_dict = {str(word).lower() : rank + 1 for rank, word in top_10k.iloc[:, 0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:46.183299Z",
     "start_time": "2018-12-27T12:18:46.177669Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = data[\"text\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:46.910469Z",
     "start_time": "2018-12-27T12:18:46.647826Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = texts.apply(lambda x: x.lower())\n",
    "texts = texts.apply(lambda x: clean_apostrophe(x))\n",
    "texts = texts.apply(lambda x: remove_punctuation(x))\n",
    "texts = texts.apply(lambda x: fix_specific(x))\n",
    "#texts = texts.apply(lambda x: clean_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:47.684114Z",
     "start_time": "2018-12-27T12:18:47.491490Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = texts.apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "oov = check_coverage(vocab, top_10k_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:49.044777Z",
     "start_time": "2018-12-27T12:18:49.039784Z"
    }
   },
   "outputs": [],
   "source": [
    "specific_vocab = [w for w, _ in oov[:1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Take a first look at the data and generate summary tables for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:59:52.678844Z",
     "start_time": "2018-12-23T11:59:52.652698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:01:00.181575Z",
     "start_time": "2018-12-23T12:01:00.174102Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:01:00.850871Z",
     "start_time": "2018-12-23T12:01:00.839125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[\"interesting\"].value_counts() / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:59:55.106849Z",
     "start_time": "2018-12-23T11:59:55.063266Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_numeric = (data[[\"claps\", \"reading_time\"]]\n",
    "                   .describe()\n",
    "                   .round(2))\n",
    "summary_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:00:35.453860Z",
     "start_time": "2018-12-23T12:00:35.443237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_numeric, name=\"summary_numeric\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:00:37.334203Z",
     "start_time": "2018-12-23T12:00:37.284924Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_object = data[[\"author\", \"title\", \"text\"]].describe()\n",
    "summary_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:00:39.077237Z",
     "start_time": "2018-12-23T12:00:39.064365Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_object, name=\"summary_object\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Exploratory visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T21:53:37.771637Z",
     "start_time": "2018-11-26T21:53:37.752286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_base = data[[\"claps\", \"reading_time\", \"interesting\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T17:58:53.443251Z",
     "start_time": "2018-11-25T17:58:52.991654Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_index = 0\n",
    "y_index = 1\n",
    "target_names = [\"not interesting\", \"interesting\"]\n",
    "\n",
    "colors = [\"red\", \"green\"]\n",
    "\n",
    "for label, color in zip(range(len(data_base[\"interesting\"])), colors):\n",
    "    plt.scatter(np.array(data_base[data_base[\"interesting\"]==label].iloc[:, x_index]), \n",
    "                np.array(data_base[data_base[\"interesting\"]==label].iloc[:, y_index]),\n",
    "                label=target_names[label],\n",
    "                c=color)\n",
    "\n",
    "plt.xlabel(data_base.columns[x_index])\n",
    "plt.ylabel(data_base.columns[y_index])\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.savefig(Path.cwd() / \"reports\" / \"images\" / \"base_classifier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a baseline model using just the numerical features `claps` and `reading time` based on three classes of classification models:\n",
    "\n",
    "- Random forests\n",
    "- Support vector machines\n",
    "- Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:06.116661Z",
     "start_time": "2018-12-24T11:30:06.110196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_cols = [\"claps\", \"reading_time\"]\n",
    "X_train_num, X_test_num = np.array(X_train[num_cols]), np.array(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:14:44.769028Z",
     "start_time": "2018-12-27T12:14:44.762953Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20,\n",
    "                            min_samples_leaf=3,\n",
    "                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:11.168499Z",
     "start_time": "2018-12-24T11:30:10.952787Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_rfs = fit_ensemble(rf, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:12.000119Z",
     "start_time": "2018-12-24T11:30:11.957255Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:13.412586Z",
     "start_time": "2018-12-24T11:30:13.380039Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_rf = evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:16.804100Z",
     "start_time": "2018-12-24T11:30:16.799860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"auto\",\n",
    "          probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:17.181887Z",
     "start_time": "2018-12-24T11:30:17.142762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_svcs = fit_ensemble(svc, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:17.487881Z",
     "start_time": "2018-12-24T11:30:17.467916Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:17.762578Z",
     "start_time": "2018-12-24T11:30:17.746579Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_svc = evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_svc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:18.739006Z",
     "start_time": "2018-12-24T11:30:18.734621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:19.000378Z",
     "start_time": "2018-12-24T11:30:18.957678Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_lrs = fit_ensemble(lr, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:19.189047Z",
     "start_time": "2018-12-24T11:30:19.171851Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:19.378905Z",
     "start_time": "2018-12-24T11:30:19.364358Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_lr = evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Collect and save baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:20.379672Z",
     "start_time": "2018-12-24T11:30:20.373416Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results = pd.concat([base_results_rf, base_results_svc, base_results_lr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:20.649980Z",
     "start_time": "2018-12-24T11:30:20.629431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T11:30:26.704745Z",
     "start_time": "2018-12-24T11:30:26.687614Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=base_results, name=\"summary_baseline_results\", out=OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare feature array for training text based models by extracting just the column containing the blog posts' text from `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:55.606140Z",
     "start_time": "2018-12-27T12:18:55.597968Z"
    }
   },
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "X_train_text, X_test_text = np.array(X_train[text_col]), np.array(X_test[text_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### CountVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scikit-learn's `CountVectorizer` is the simplest approach to turning the blog posts' texts into numerical matrices. It will just count the number of occurences of each token in the text and create a sparse matrix holding these counts for all posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let's do everything with default values to get a general feeling for how this approach performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:05.546662Z",
     "start_time": "2018-12-24T12:41:05.542368Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer_specific = CountVectorizer(vocabulary=specific_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:05.950701Z",
     "start_time": "2018-12-24T12:41:05.946740Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:08.868294Z",
     "start_time": "2018-12-24T12:41:08.861295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf = make_pipeline(count_vectorizer, RandomForestClassifier(n_estimators=10, random_state=1, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:13.339076Z",
     "start_time": "2018-12-24T12:41:09.177933Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_countvec_rf = fit_ensemble(pipe_countvec_rf, sss, X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:14.362143Z",
     "start_time": "2018-12-24T12:41:13.341576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_countvec_rf_full = evaluate_ensemble(fitted_countvec_rf, evaluator, X_test_text, y_test,\n",
    "                                         return_res=True, method=\"countvec rf full v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only top k words specific to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:18.660400Z",
     "start_time": "2018-12-24T12:41:14.369379Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf_specific = make_pipeline(count_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                            random_state=1,\n",
    "                                                                                            n_jobs=-1))\n",
    "fitted_countvec_rf_specific = fit_ensemble(pipe_countvec_rf_specific, sss, X_train_text, y_train)\n",
    "res_countvec_rf_specific = evaluate_ensemble(fitted_countvec_rf_specific, evaluator, X_test_text, y_test, return_res=True, method=\"countvec rf specific v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:18.668603Z",
     "start_time": "2018-12-24T12:41:18.663726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc = make_pipeline(count_vectorizer, SVC(gamma=\"auto\", probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:21.386968Z",
     "start_time": "2018-12-24T12:41:18.671855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_countvec_svc = fit_ensemble(pipe_countvec_svc, sss, X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:21.824014Z",
     "start_time": "2018-12-24T12:41:21.389695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_countvec_svc_full = evaluate_ensemble(fitted_countvec_svc, evaluator, X_test_text, y_test, return_res=True, method=\"countvec svc full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:23.574312Z",
     "start_time": "2018-12-24T12:41:21.827230Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc_specific = make_pipeline(count_vectorizer_specific, SVC(gamma=\"auto\", probability=True))\n",
    "fitted_countvec_svc_specific = fit_ensemble(pipe_countvec_svc_specific, sss, X_train_text, y_train)\n",
    "res_countvec_svc_specific = evaluate_ensemble(fitted_countvec_svc_specific, evaluator, X_test_text, y_test, return_res=True, method=\"countvec svc specific\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Some optimization (preprocessing, feature selection, model tuning) using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:23.591254Z",
     "start_time": "2018-12-24T12:41:23.577825Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer(vocabulary=specific_vocab)),\n",
    "    (\"rf\", RandomForestClassifier())\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=9000),\n",
    "          \"rf__n_estimators\": randint(low=10, high=40),\n",
    "          \"rf__max_depth\": randint(low=2, high=8),\n",
    "          \"rf__min_samples_leaf\": randint(low=1, high=10),\n",
    "          \"rf__max_features\": [0.5, \"sqrt\", \"auto\"]}\n",
    "\n",
    "grid_1 = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=10,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          cv=5,\n",
    "                          return_train_score=False)\n",
    "\n",
    "grid_2 = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=10,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          cv=5,\n",
    "                          return_train_score=False)\n",
    "\n",
    "grid_3 = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=10,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:45.656322Z",
     "start_time": "2018-12-24T12:41:23.594787Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = grid_1.fit(X_train_text, y_train)\n",
    "_ = grid_2.fit(X_train_text, y_train)\n",
    "_ = grid_3.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:46.345017Z",
     "start_time": "2018-12-24T12:41:45.660192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator_1 = grid_1.best_estimator_.fit(X_train_text, y_train)\n",
    "estimator_2 = grid_2.best_estimator_.fit(X_train_text, y_train)\n",
    "estimator_3 = grid_3.best_estimator_.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:46:37.118287Z",
     "start_time": "2018-12-23T12:46:37.082756Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:46:37.356334Z",
     "start_time": "2018-12-23T12:46:37.125931Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([estimator_1], evaluator, X_test_text, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:46:37.371688Z",
     "start_time": "2018-12-23T12:46:37.360103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:46:37.627162Z",
     "start_time": "2018-12-23T12:46:37.375992Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([estimator_2], evaluator, X_test_text, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:46:37.656496Z",
     "start_time": "2018-12-23T12:46:37.639872Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "estimator_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:46:37.796796Z",
     "start_time": "2018-12-23T12:46:37.670409Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([estimator_3], evaluator, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T12:46:38.419034Z",
     "start_time": "2018-12-23T12:46:37.803570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([estimator_1, estimator_2, estimator_3], evaluator, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### TfidfVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next approach to feature extraction from the text data that I want to try is the `Term-Frequency-Inverse-Document-Frequency` technique implemented in scikit-learn's `TfidfVectorizer`. This method creates the same matrix as the `CountVectorizer` but divides the values for each token in the vocabulary by its frequency across all documents in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:50.909142Z",
     "start_time": "2018-12-24T12:41:46.347716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "pipe_tfidf = make_pipeline(tfidf_vectorizer, RandomForestClassifier(n_estimators=10,\n",
    "                                                                    random_state=1,\n",
    "                                                                    n_jobs=-1))\n",
    "fitted_tfidf = fit_ensemble(pipe_tfidf, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_full = evaluate_ensemble(fitted_tfidf, evaluator, X_test_text, y_test, return_res=True, method=\"tfidf rf full v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only specific vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:55.151075Z",
     "start_time": "2018-12-24T12:41:50.912793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_specific = TfidfVectorizer(vocabulary=specific_vocab)\n",
    "pipe_tfidf_specific = make_pipeline(tfidf_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                      random_state=1,\n",
    "                                                                                      n_jobs=-1))\n",
    "fitted_tfidf_specific = fit_ensemble(pipe_tfidf_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_specific = evaluate_ensemble(fitted_tfidf_specific, evaluator, X_test_text, y_test, return_res=True, method=\"tfidf rf specific v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:55.173526Z",
     "start_time": "2018-12-24T12:41:55.159190Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc = make_pipeline(tfidf_vectorizer, SVC(gamma=\"auto\",\n",
    "                                                     C=0.8,\n",
    "                                                     probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:57.819322Z",
     "start_time": "2018-12-24T12:41:55.182915Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_tfidf_svc = fit_ensemble(pipe_tfidf_svc, sss, X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:41:58.282286Z",
     "start_time": "2018-12-24T12:41:57.821885Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_tfidf_svc_full = evaluate_ensemble(fitted_tfidf_svc, evaluator, X_test_text, y_test, return_res=True, method=\"tfidf svc full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:42:00.059755Z",
     "start_time": "2018-12-24T12:41:58.286578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc_specific = make_pipeline(tfidf_vectorizer_specific, SVC(gamma=\"auto\",\n",
    "                                                     C=0.8,\n",
    "                                                     probability=True))\n",
    "fitted_tfidf_svc_specific = fit_ensemble(pipe_tfidf_svc_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_svc_specific = evaluate_ensemble(fitted_tfidf_svc_specific, evaluator, X_test_text, y_test,\n",
    "                                           return_res=True, method=\"tfidf svc full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:42:00.071074Z",
     "start_time": "2018-12-24T12:42:00.063479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec = pd.concat([res_countvec_rf_full, res_countvec_rf_specific,\n",
    "                                    res_countvec_svc_full, res_countvec_svc_specific,\n",
    "                                    res_tfidf_full, res_tfidf_rf_full,\n",
    "                                    res_tfidf_rf_specific, res_tfidf_svc_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:42:00.096992Z",
     "start_time": "2018-12-24T12:42:00.074451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:42:00.127702Z",
     "start_time": "2018-12-24T12:42:00.101117Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=results_tfidf_countvec, name=\"summary_countvec_tfidf_results\", out=OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:29:56.719679Z",
     "start_time": "2018-12-24T12:29:56.658222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", TfidfVectorizer()),\n",
    "    (\"rf\", RandomForestClassifier())\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=9000),\n",
    "          \"rf__n_estimators\": randint(low=10, high=40),\n",
    "          \"rf__max_depth\": randint(low=2, high=8),\n",
    "          \"rf__min_samples_leaf\": randint(low=1, high=10),\n",
    "          \"rf__max_features\": [0.5, \"sqrt\", \"auto\"]}\n",
    "\n",
    "grid_1_tfidf = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=10,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          random_state=1,\n",
    "                          cv=5,\n",
    "                          return_train_score=False)\n",
    "\n",
    "grid_2_tfidf = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=10,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          random_state=1,\n",
    "                          cv=5,\n",
    "                          return_train_score=False)\n",
    "\n",
    "grid_3_tfidf = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=10,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          n_jobs=-1,\n",
    "                          random_state=1,\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:30:56.239957Z",
     "start_time": "2018-12-24T12:30:01.429609Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_ = grid_1_tfidf.fit(X_train_text, y_train)\n",
    "_ = grid_2_tfidf.fit(X_train_text, y_train)\n",
    "_ = grid_3_tfidf.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:31:25.317114Z",
     "start_time": "2018-12-24T12:31:25.117343Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([grid_1_tfidf.best_estimator_], evaluator, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:31:26.542431Z",
     "start_time": "2018-12-24T12:31:26.377733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([grid_2_tfidf.best_estimator_], evaluator, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:31:27.596378Z",
     "start_time": "2018-12-24T12:31:27.345585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([grid_3_tfidf.best_estimator_], evaluator, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T12:31:42.196965Z",
     "start_time": "2018-12-24T12:31:41.661605Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble([grid_1_tfidf.best_estimator_, grid_2_tfidf.best_estimator_, grid_3_tfidf.best_estimator_],\n",
    "                  evaluator,\n",
    "                  X_test_text,\n",
    "                  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained word embeddings + neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is heavily based on these two Kaggle Kernels:\n",
    "- [Processing text when using word embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook)\n",
    "- [Comparing word embeddings](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:18:59.239813Z",
     "start_time": "2018-12-27T12:18:59.233977Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_LEN = 1000 # max number of words in a blog post\n",
    "EMBEDDING_PATH = Path.cwd() / \"resources\" / \"embeddings\"\n",
    "EMBEDDING_FOLDER = EMBEDDING_PATH / \"glove.840B.300d\"\n",
    "SPECIFIC_ONLY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Purpose specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:00.022081Z",
     "start_time": "2018-12-27T12:19:00.009862Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:06.416566Z",
     "start_time": "2018-12-27T12:19:06.412797Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:36.079401Z",
     "start_time": "2018-12-27T12:19:08.152485Z"
    }
   },
   "outputs": [],
   "source": [
    "if (EMBEDDING_FOLDER / \"embeddings_index.pkl\").is_file():\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"rb\") as handle:\n",
    "        embeddings_index = pickle.load(handle)\n",
    "else:\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FOLDER / \"glove.840B.300d.txt\"))\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"wb\") as handle:\n",
    "        pickle.dump(embeddings_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess the text data to work well with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:37.169096Z",
     "start_time": "2018-12-27T12:19:37.163755Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train_text.copy())\n",
    "X_test = pd.Series(X_test_text.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:37.765643Z",
     "start_time": "2018-12-27T12:19:37.602454Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: x.lower())\n",
    "X_train = X_train.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_train.apply(lambda x: fix_punctuation(x))\n",
    "X_train = X_train.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the same transformations to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:38.667991Z",
     "start_time": "2018-12-27T12:19:38.539572Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = X_test.apply(lambda x: x.lower())\n",
    "X_test = X_test.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_test.apply(lambda x: fix_punctuation(x))\n",
    "X_test = X_test.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:41.669461Z",
     "start_time": "2018-12-27T12:19:41.269467Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:42.531766Z",
     "start_time": "2018-12-27T12:19:42.065181Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:19:42.964560Z",
     "start_time": "2018-12-27T12:19:42.943719Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process embeddings into a matrix of size `(max_features, embed_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:20:06.950691Z",
     "start_time": "2018-12-27T12:19:44.080535Z"
    }
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:20:07.432394Z",
     "start_time": "2018-12-27T12:20:06.956945Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "if SPECIFIC_ONLY: word_index = {word : i for word, i in word_index.items() if word in specific_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:20:07.841191Z",
     "start_time": "2018-12-27T12:20:07.437750Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_FEATURES, len(tokenizer.word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:20:09.359893Z",
     "start_time": "2018-12-27T12:20:07.848952Z"
    }
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAX_LEN,))\n",
    "x = Embedding(MAX_FEATURES, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T22:20:54.207153Z",
     "start_time": "2018-11-26T22:20:54.193978Z"
    }
   },
   "source": [
    "##### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.training import Model as keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:29:37.200262Z",
     "start_time": "2018-12-27T12:24:34.600022Z"
    }
   },
   "outputs": [],
   "source": [
    "fitted_keras = fit_ensemble(model, sss, X_train, y_train, print_progress=True,\n",
    "                            batch_size=16, epochs=3, verbose=0, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:04:45.780095Z",
     "start_time": "2018-12-27T12:03:14.383672Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=8,\n",
    "          epochs=5,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:29:52.371953Z",
     "start_time": "2018-12-27T12:29:52.366446Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.engine.training import Model as keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:32:27.153693Z",
     "start_time": "2018-12-27T12:32:27.143357Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_ensemble(fitted:List, eval:CustomEvaluator, X_test:ndarray,\n",
    "                      y_test:ndarray, return_res:bool=False, method:str=\"default\") -> pd.DataFrame:\n",
    "    \"\"\"Evaluate the performance of a set of classifiers trained on different subsets of the training set\n",
    "    \n",
    "    Arguments:\n",
    "    fitted - list of named tuples containing fitted models as well as their train and out of bag AUC\n",
    "    eval - object of class CustomEvaluator used to evaluate the performance on the hold out set\n",
    "    X_test - numpy array of the texts for the hold out set for final evaluation\n",
    "    y_test - numpy array of labels for the hold out set for final evaluation\n",
    "    \n",
    "    Return:\n",
    "    pd.DataFrame - if requested, return pandas dataframe summarizing the results\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(fitted[0], \"clf\"):\n",
    "        train_scores = [m.train_auc for m in fitted]\n",
    "        oob_scores = [m.oob_auc for m in fitted]\n",
    "        \n",
    "        if hasattr(fitted[0].clf, \"predict_proba\"):\n",
    "            preds_test = np.array([m.clf.predict_proba(X_test)[:, 1] for m in fitted])\n",
    "        elif isinstance(fitted[0].clf, keras_model):\n",
    "            preds_test = np.array([m.clf.predict(X_test) for m in fitted])\n",
    "        \n",
    "        print(f\"Mean Train AUC: {np.mean(train_scores):.2f} (+/- {np.std(train_scores):.2f})\")\n",
    "        print(f\"Mean OOB AUC: {np.mean(oob_scores):.2f} (+/- {np.std(oob_scores):.2f})\")\n",
    "        print(\"\")\n",
    "        \n",
    "    else: preds_test = np.array([m.predict_proba(X_test)[:, 1] for m in fitted])\n",
    "    print(\"Performance on hold out set:\")\n",
    "    if return_res:\n",
    "        test_auc = eval.score(y_test, preds_test.mean(axis=0), return_res)\n",
    "        return pd.DataFrame({\"method\": method,\n",
    "                             \"mean train auc\" : np.mean(train_scores),\n",
    "                                 \"mean cv auc\" : np.mean(oob_scores),\n",
    "                             \"mean test auc\" : test_auc}, index=[0])\n",
    "    else:\n",
    "        eval.score(y_test, preds_test.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T12:32:36.924386Z",
     "start_time": "2018-12-27T12:32:29.506686Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_keras, evaluator, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### SpaCy language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Instructions from SpaCy documentation](https://spacy.io/usage/training#section-textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T10:20:47.301752Z",
     "start_time": "2018-11-25T10:20:47.262793Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CustomSpacyClassifier():\n",
    "    \"\"\" Wrapper for spaCy's text classification that enables integration with sklearn.metrics.cross_validate\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._estimator_type = \"classifier\"\n",
    "        \n",
    "        self.nlp = None\n",
    "        self.label = None\n",
    "        self.train_data = None\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return dict()\n",
    "    \n",
    "    def add_textcat(self, label):\n",
    "        self.label = label\n",
    "        if \"textcat\" not in self.nlp.pipe_names:\n",
    "            textcat = self.nlp.create_pipe(\"textcat\")\n",
    "            self.nlp.add_pipe(textcat, last=True)\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        else:\n",
    "            textcat = self.nlp.get_pipe(\"textcat\")\n",
    "        textcat.add_label(label)\n",
    "    \n",
    "    def fit(self, X, y, n_iter=10, **kwargs):\n",
    "        \n",
    "        self.nlp = spacy.load(\"en\")\n",
    "        self.add_textcat(\"interesting\")\n",
    "        self.train_data = [(e, {\"cats\": {self.label: bool(l)}}) for e, l in zip(X, y)]\n",
    "        \n",
    "        drop_rate = kwargs[\"drop_rate\"]\n",
    "        \n",
    "        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != \"textcat\"]\n",
    "        with self.nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "            optimizer = self.nlp.begin_training()\n",
    "            for i in range(n_iter):\n",
    "                print(f\"EPOCH {i+1}\")\n",
    "                losses = {}\n",
    "                batches = minibatch(self.train_data, size=compounding(4., 16., 1.001))\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    self.nlp.update(texts, annotations, sgd=optimizer, drop=drop_rate,\n",
    "                               losses=losses)\n",
    "                loss = losses[\"textcat\"]\n",
    "                print(f\"LOSS: {loss}\")\n",
    "                print(\"\")\n",
    "                \n",
    "    def predict_proba(self, X):\n",
    "        p1_scores = [np.float64(self.nlp(sample_text).cats[\"interesting\"]) for sample_text in X]\n",
    "        \n",
    "        return np.array([[1. - score, score] for score in p1_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T10:20:51.681830Z",
     "start_time": "2018-11-25T10:20:49.815290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T10:20:53.556415Z",
     "start_time": "2018-11-25T10:20:53.535100Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf_spacy = CustomSpacyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T10:34:31.937623Z",
     "start_time": "2018-11-25T10:20:56.981637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_spacy = fit_ensemble(clf_spacy, sss, X_train_text, y_train, n_iter=5, drop_rate=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T10:37:31.478364Z",
     "start_time": "2018-11-25T10:34:31.944130Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_spacy, evaluator, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T13:37:33.399044Z",
     "start_time": "2018-11-25T13:35:46.324635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for m in fitted_spacy:\n",
    "    evaluate_ensemble([m], evaluator, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T14:18:13.636961Z",
     "start_time": "2018-11-18T14:18:08.826478Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T14:26:39.926099Z",
     "start_time": "2018-11-18T14:26:39.882742Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path.cwd() / \"data\" / \"shared\" / \"train_data_fastai.csv\",\n",
    "                   header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T14:26:41.580040Z",
     "start_time": "2018-11-18T14:26:41.565399Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Train and evaluate using custom metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.columns = [\"label\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T14:24:13.458848Z",
     "start_time": "2018-11-18T14:24:13.454103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH = Path.cwd() / \"data\" / \"shared\" / \"fastai\"\n",
    "os.makedirs(PATH / \"exp\", exist_ok=True)\n",
    "EXP_PATH = PATH / \"exp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Train and Evaluate the Language Model (Metric : Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T14:26:59.235384Z",
     "start_time": "2018-11-18T14:26:52.772832Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(df=data, path=EXP_PATH, cols=\"text\")\n",
    "             .random_split_by_pct()\n",
    "             .label_for_lm()\n",
    "             .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T14:29:03.974583Z",
     "start_time": "2018-11-18T14:28:05.094071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-18T14:29:21.347Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, 5e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"fit_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"fit_head\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, 5e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"fine_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save_encoder(\"fine_tuned_enc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Train and Evaluate the Classifier (Metric = F_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_clf = (TextList.from_df(df=data, path=EXP_PATH, cols=[\"text\"], vocab=data_lm.vocab)\n",
    "               .random_split_by_pct()\n",
    "               .label_from_df(cols=\"label\")\n",
    "               .databunch(bs=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "My initial objective was to achieve `precision = 0.95` and `recall = 0.75`. As `0.75 / 0.95` is approx. `0.8`, I will use the fbeta score with `beta = 0.8` to evaluate my classifier. Let's calculate the benchmark first:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f_beta(beta, pr, rc):\n",
    "    beta2 = beta**2\n",
    "    return (1+beta2) * pr*rc / (beta2*pr + rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_beta(0.8, 0.95, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FBetaBinary(Callback):\n",
    "    \"Computes the f_beta between preds and targets for binary classification\"\n",
    "\n",
    "    def __init__(self, beta=1, eps=1e-9, sigmoid=True, thresh=0.5):      \n",
    "        self.beta2 = beta**2\n",
    "        self.eps = eps\n",
    "        self.sigmoid = sigmoid\n",
    "        self.thresh = thresh\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.TP = 0\n",
    "        self.total_y_pred = 0   \n",
    "        self.total_y_true = 0\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        y_pred = last_output\n",
    "        y_pred = y_pred.softmax(dim=1)\n",
    "        y_pred = (y_pred[:, 1]>self.thresh).float()\n",
    "        y_true = last_target.float()\n",
    "        \n",
    "        self.TP += ((y_pred==1) * (y_true==1)).float().sum()\n",
    "        self.total_y_pred += (y_pred==1).float().sum()\n",
    "        self.total_y_true += (y_true==1).float().sum()\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        prec = self.TP / (self.total_y_pred+self.eps)\n",
    "        rec = self.TP / (self.total_y_true+self.eps)\n",
    "        res = (prec*rec) / (prec*self.beta2+rec+self.eps) * (1+self.beta2)        \n",
    "        self.metric = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for t in np.arange(0.1, 0.4, 0.05):\n",
    "    metrics.append(FBetaBinary(beta=0.8, thresh=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clf, drop_mult=0.5)\n",
    "learn.load_encoder(\"fine_tuned_enc\")\n",
    "learn.metrics = metrics\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 5e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"first\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"second\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"three\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load(\"three\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(5e-4/(2.6**4),5e-4), moms=(0.8,0.7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:medium-classifier]",
   "language": "python",
   "name": "conda-env-medium-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
