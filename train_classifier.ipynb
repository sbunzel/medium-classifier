{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:27.424567Z",
     "start_time": "2019-01-04T21:30:23.232070Z"
    }
   },
   "outputs": [],
   "source": [
    "# file system navigation\n",
    "from pathlib import Path\n",
    "\n",
    "# data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ml algorithms and evaluation metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import scipy\n",
    "from scipy.stats.distributions import uniform, randint\n",
    "\n",
    "# sklearn specifics\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.util import decaying\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# misc\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:27.434755Z",
     "start_time": "2019-01-04T21:30:27.426922Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:27.846392Z",
     "start_time": "2019-01-04T21:30:27.842310Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path.cwd() / \"data\" / \"shared\"\n",
    "OUTPUT_PATH = Path.cwd() / \"reports\" / \"images-and-tables\"\n",
    "MODEL_PATH = Path.cwd() / \"models\"\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:28.196493Z",
     "start_time": "2019-01-04T21:30:28.191865Z"
    }
   },
   "outputs": [],
   "source": [
    "if not OUTPUT_PATH.is_dir():\n",
    "    OUTPUT_PATH.mkdir(parents=True)\n",
    "    \n",
    "if not MODEL_PATH.is_dir():\n",
    "    MODEL_PATH.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Helper visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generate a plot of interest in Machine Learning over time based on Google trends data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:37.344338Z",
     "start_time": "2019-01-04T17:41:37.326121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml = pd.read_csv(Path.cwd() / \"data\" / \"trends-ml.csv\",\n",
    "                               skiprows=3,\n",
    "                               header=None,\n",
    "                               names=[\"date\", \"interest\"],\n",
    "                               parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:37.363697Z",
     "start_time": "2019-01-04T17:41:37.346854Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "google_trends_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:37.584114Z",
     "start_time": "2019-01-04T17:41:37.367165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interest_plot = google_trends_ml.plot(x=\"date\",\n",
    "                      y=\"interest\",\n",
    "                      legend=False)\n",
    "interest_plot.set_xlabel(\"Date\")\n",
    "interest_plot.set_ylabel(\"Relative interest\")\n",
    "interest_plot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.310648Z",
     "start_time": "2019-01-04T17:41:37.587849Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = interest_plot.get_figure()\n",
    "fig.savefig(Path.cwd() / \"reports\" / \"images\" / \"interest-in-ml.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preprocessed data and split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:30.335599Z",
     "start_time": "2019-01-04T21:30:30.203628Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(DATA_PATH / \"train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:30.410747Z",
     "start_time": "2019-01-04T21:30:30.401101Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data[[\"claps\", \"reading_time\", \"text\"]]\n",
    "y = np.array(data[\"interesting\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3, random_state=SEED,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize custom evaluator and shuffle split generator to use across all modeling approaches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:31.146765Z",
     "start_time": "2019-01-04T21:30:31.141686Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = CustomEvaluator(target_precision=0.8)\n",
    "sss = model_selection.StratifiedShuffleSplit(n_splits=6, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Generate a vocabulary of words specific to the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use a list of the top 10k most frequent words in the English language obtained from [this repository](https://github.com/first20hours/google-10000-english) to identify the words specific to the corpus of block posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:30:21.135916Z",
     "start_time": "2019-01-04T18:30:21.106107Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TOP_WORDS_PATH = Path.cwd() / \"resources\" / \"top_words\"\n",
    "top_10k = pd.read_table(TOP_WORDS_PATH / \"google-10000-english\" / \"google-10000-english.txt\", header=None)\n",
    "top_10k_dict = {str(word).lower() : rank + 1 for rank, word in top_10k.iloc[:, 0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:30:21.447697Z",
     "start_time": "2019-01-04T18:30:21.441239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = data[\"text\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:30:21.942789Z",
     "start_time": "2019-01-04T18:30:21.790610Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = texts.apply(lambda x: x.lower())\n",
    "texts = texts.apply(lambda x: clean_apostrophe(x))\n",
    "texts = texts.apply(lambda x: remove_punctuation(x))\n",
    "texts = texts.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:30:22.288918Z",
     "start_time": "2019-01-04T18:30:22.169706Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = texts.apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "oov = check_coverage(vocab, top_10k_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:30:22.804653Z",
     "start_time": "2019-01-04T18:30:22.799469Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "specific_vocab = [w for w, _ in oov]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Take a first look at the data and generate summary tables for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.325682Z",
     "start_time": "2019-01-04T17:41:33.384Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.327063Z",
     "start_time": "2019-01-04T17:41:33.390Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.328978Z",
     "start_time": "2019-01-04T17:41:33.400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[\"interesting\"].value_counts() / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.330649Z",
     "start_time": "2019-01-04T17:41:33.408Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_numeric = (data[[\"claps\", \"reading_time\"]]\n",
    "                   .describe()\n",
    "                   .round(2))\n",
    "summary_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.332626Z",
     "start_time": "2019-01-04T17:41:33.415Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_numeric, name=\"summary_numeric\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.334060Z",
     "start_time": "2019-01-04T17:41:33.421Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_object = data[[\"author\", \"title\", \"text\"]].describe()\n",
    "summary_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.336020Z",
     "start_time": "2019-01-04T17:41:33.428Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=summary_object, name=\"summary_object\", out=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Exploratory visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.337890Z",
     "start_time": "2019-01-04T17:41:33.437Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_base = data[[\"claps\", \"reading_time\", \"interesting\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.340783Z",
     "start_time": "2019-01-04T17:41:33.445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_index = 0\n",
    "y_index = 1\n",
    "target_names = [\"not interesting\", \"interesting\"]\n",
    "\n",
    "colors = [\"red\", \"green\"]\n",
    "\n",
    "for label, color in zip(range(len(data_base[\"interesting\"])), colors):\n",
    "    plt.scatter(np.array(data_base[data_base[\"interesting\"]==label].iloc[:, x_index]), \n",
    "                np.array(data_base[data_base[\"interesting\"]==label].iloc[:, y_index]),\n",
    "                label=target_names[label],\n",
    "                c=color)\n",
    "\n",
    "plt.xlabel(data_base.columns[x_index])\n",
    "plt.ylabel(data_base.columns[y_index])\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.savefig(Path.cwd() / \"reports\" / \"images\" / \"base_classifier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a baseline model using just the numerical features `claps` and `reading time` based on three classes of classification models:\n",
    "\n",
    "- Random forests\n",
    "- Support vector machines\n",
    "- Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.342766Z",
     "start_time": "2019-01-04T17:41:33.453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_cols = [\"claps\", \"reading_time\"]\n",
    "X_train_num, X_test_num = np.array(X_train[num_cols]), np.array(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.344600Z",
     "start_time": "2019-01-04T17:41:33.465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20,\n",
    "                            min_samples_leaf=3,\n",
    "                            random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.347297Z",
     "start_time": "2019-01-04T17:41:33.482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_rfs = fit_ensemble(rf, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.349921Z",
     "start_time": "2019-01-04T17:41:33.490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.352004Z",
     "start_time": "2019-01-04T17:41:33.497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_rf = evaluate_ensemble(fitted_rfs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.354025Z",
     "start_time": "2019-01-04T17:41:33.504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"auto\",\n",
    "          probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.356172Z",
     "start_time": "2019-01-04T17:41:33.510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_svcs = fit_ensemble(svc, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.358108Z",
     "start_time": "2019-01-04T17:41:33.515Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.360537Z",
     "start_time": "2019-01-04T17:41:33.521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_svc = evaluate_ensemble(fitted_svcs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_svc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.362524Z",
     "start_time": "2019-01-04T17:41:33.527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"liblinear\", random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.364560Z",
     "start_time": "2019-01-04T17:41:33.532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_lrs = fit_ensemble(lr, sss, X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.366856Z",
     "start_time": "2019-01-04T17:41:33.537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.368857Z",
     "start_time": "2019-01-04T17:41:33.542Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results_lr = evaluate_ensemble(fitted_lrs, evaluator, X_test_num, y_test, return_res=True, method=\"baseline_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Collect and save baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.371036Z",
     "start_time": "2019-01-04T17:41:33.547Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results = pd.concat([base_results_rf, base_results_svc, base_results_lr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.373677Z",
     "start_time": "2019-01-04T17:41:33.552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.375414Z",
     "start_time": "2019-01-04T17:41:33.558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(df=base_results, name=\"summary_baseline_results\", out=OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare feature array for training text based models by extracting just the column containing the blog posts' text from `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:35.202018Z",
     "start_time": "2019-01-04T21:30:35.197622Z"
    }
   },
   "outputs": [],
   "source": [
    "text_col = \"text\"\n",
    "X_train_text, X_test_text = np.array(X_train[text_col]), np.array(X_test[text_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### CountVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scikit-learn's `CountVectorizer` is the simplest approach to turning the blog posts' texts into numerical matrices. It will just count the number of occurences of each token in the text and create a sparse matrix holding these counts for all posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let's do everything with default values to get a general feeling for how this approach performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.378301Z",
     "start_time": "2019-01-04T17:41:33.570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer_specific = CountVectorizer(vocabulary=specific_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.380712Z",
     "start_time": "2019-01-04T17:41:33.576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.382044Z",
     "start_time": "2019-01-04T17:41:33.585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf = make_pipeline(count_vectorizer, RandomForestClassifier(n_estimators=10,\n",
    "                                                                          random_state=SEED,\n",
    "                                                                          n_jobs=-1))\n",
    "fitted_countvec_rf = fit_ensemble(pipe_countvec_rf, sss, X_train_text, y_train)\n",
    "res_countvec_rf_full = evaluate_ensemble(fitted_countvec_rf, evaluator, X_test_text, y_test,\n",
    "                                         return_res=True, method=f\"countvec rf full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.384511Z",
     "start_time": "2019-01-04T17:41:33.591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_rf_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only top k words specific to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.387046Z",
     "start_time": "2019-01-04T17:41:33.596Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_rf_specific = make_pipeline(count_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                            random_state=1,\n",
    "                                                                                            n_jobs=-1))\n",
    "fitted_countvec_rf_specific = fit_ensemble(pipe_countvec_rf_specific, sss, X_train_text, y_train)\n",
    "res_countvec_rf_specific = evaluate_ensemble(fitted_countvec_rf_specific, evaluator, X_test_text, y_test,\n",
    "                                             return_res=True, method=f\"countvec rf specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.388855Z",
     "start_time": "2019-01-04T17:41:33.602Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_rf_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use scikit-learn's `StandardScaler` here as the SVC's default kernel (`rbf`) expects normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.391183Z",
     "start_time": "2019-01-04T17:41:33.608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc = make_pipeline(count_vectorizer, StandardScaler(with_mean=False),\n",
    "                                  SVC(gamma=\"auto\", probability=True, random_state=SEED))\n",
    "fitted_countvec_svc = fit_ensemble(pipe_countvec_svc, sss, X_train_text, y_train)\n",
    "res_countvec_svc_full = evaluate_ensemble(fitted_countvec_svc, evaluator, X_test_text, y_test,\n",
    "                                          return_res=True, method=f\"countvec svc full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.392940Z",
     "start_time": "2019-01-04T17:41:33.613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.395480Z",
     "start_time": "2019-01-04T17:41:33.618Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_countvec_svc_specific = make_pipeline(count_vectorizer_specific,\n",
    "                                           StandardScaler(with_mean=False),\n",
    "                                           SVC(gamma=\"auto\", probability=True, random_state=SEED))\n",
    "fitted_countvec_svc_specific = fit_ensemble(pipe_countvec_svc_specific, sss, X_train_text, y_train)\n",
    "res_countvec_svc_specific = evaluate_ensemble(fitted_countvec_svc_specific, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"countvec svc specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.397584Z",
     "start_time": "2019-01-04T17:41:33.623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Grid search on best default models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Full vocab + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.399651Z",
     "start_time": "2019-01-04T17:41:33.629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer()),\n",
    "    (\"std\", StandardScaler(with_mean=False)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=20000),\n",
    "          \"svc__C\": scipy.stats.expon(scale=1.0),\n",
    "          \"svc__gamma\": [\"auto\", \"scale\"],\n",
    "          \"svc__kernel\": [\"rbf\"],\n",
    "          \"svc__class_weight\": [\"balanced\", None]}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=50,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.401793Z",
     "start_time": "2019-01-04T17:41:33.635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_fitted = grid.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.404626Z",
     "start_time": "2019-01-04T17:41:33.641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(grid_fitted.best_estimator_, MODEL_PATH / \"countvec_full_svc_grid_best.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.407247Z",
     "start_time": "2019-01-04T17:41:33.647Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_full_grid_best = joblib.load(MODEL_PATH / \"countvec_full_svc_grid_best.pkl\")\n",
    "countvec_svc_full_grid_best = countvec_svc_full_grid_best.set_params(svc__random_state=SEED)\n",
    "fitted_countvec_svc_full_grid_best = fit_ensemble(countvec_svc_full_grid_best, sss,\n",
    "                                                X_train_text, y_train, print_progress=True)\n",
    "res_countvec_svc_full_best = evaluate_ensemble(fitted_countvec_svc_full_grid_best, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"countvec svc full best params v{SEED}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:59:03.343861Z",
     "start_time": "2019-01-04T17:59:03.330044Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_full_grid_best.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.413226Z",
     "start_time": "2019-01-04T17:41:33.658Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_full_best, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Specific vocab + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:42:18.705334Z",
     "start_time": "2019-01-04T17:42:18.690244Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"vec\", CountVectorizer(vocabulary=specific_vocab)),\n",
    "    (\"std\", StandardScaler(with_mean=False)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "    ])\n",
    "params = {\"vec__stop_words\": [\"english\", None],\n",
    "          \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "          \"vec__max_df\": uniform(loc=0.8, scale=0.2),\n",
    "          \"vec__min_df\": uniform(loc=0.0, scale=0.2),\n",
    "          \"vec__max_features\": randint(low=1000, high=20000),\n",
    "          \"svc__C\": scipy.stats.expon(scale=1.0),\n",
    "          \"svc__gamma\": [\"auto\", \"scale\"],\n",
    "          \"svc__kernel\": [\"rbf\"],\n",
    "          \"svc__class_weight\": [\"balanced\", None]}\n",
    "\n",
    "grid = RandomizedSearchCV(pipe,\n",
    "                          params,\n",
    "                          n_iter=50,\n",
    "                          scoring=\"roc_auc\",\n",
    "                          cv=5,\n",
    "                          return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:43:22.605494Z",
     "start_time": "2019-01-04T17:42:24.012035Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_fitted = grid.fit(X_train_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:44:10.925330Z",
     "start_time": "2019-01-04T17:44:10.767530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(grid_fitted.best_estimator_, MODEL_PATH / \"countvec_specific_svc_grid_best.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:47.470853Z",
     "start_time": "2019-01-04T17:46:43.867196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_specific_grid_best = joblib.load(MODEL_PATH / \"countvec_specific_svc_grid_best.pkl\")\n",
    "countvec_svc_specific_grid_best = countvec_svc_specific_grid_best.set_params(svc__random_state=SEED)\n",
    "fitted_countvec_svc_specific_grid_best = fit_ensemble(countvec_svc_specific_grid_best, sss,\n",
    "                                                X_train_text, y_train, print_progress=True)\n",
    "res_countvec_svc_specific_best = evaluate_ensemble(fitted_countvec_svc_specific_grid_best, evaluator, X_test_text, y_test,\n",
    "                                              return_res=True, method=f\"best params countvec svc specific v{SEED}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:56:42.562406Z",
     "start_time": "2019-01-04T17:56:42.538454Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countvec_svc_specific_grid_best.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:50.376285Z",
     "start_time": "2019-01-04T17:46:50.362907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_countvec_svc_specific_best, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Create summary of grid search results and save to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:46:55.905856Z",
     "start_time": "2019-01-04T17:46:55.874298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_countvec_grid = read_results(OUTPUT_PATH / \"raw\", \"best_params_countvec*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:47:49.569125Z",
     "start_time": "2019-01-04T17:47:49.562156Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res = pd.concat(list_countvec_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:01.571932Z",
     "start_time": "2019-01-04T17:48:01.553694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "res_groupby_method = res.groupby(\"base_method\")\n",
    "res_analysis_table = res_groupby_method.agg({c : [\"mean\", \"std\"] for c in [\"mean train auc\", \"mean cv auc\", \"mean test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:13.400238Z",
     "start_time": "2019-01-04T17:48:13.391141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out = res_analysis_table.sort_values([(\"mean test auc\", \"mean\"), (\"mean cv auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:22.758106Z",
     "start_time": "2019-01-04T17:48:22.750427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"cv auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:48:26.219536Z",
     "start_time": "2019-01-04T17:48:26.198324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:49:13.765812Z",
     "start_time": "2019-01-04T17:49:13.734172Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_html(out, \"summary_countvec_svc_best_params\", out=OUTPUT_PATH, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer + Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next approach to feature extraction from the text data that I want to try is the `Term-Frequency-Inverse-Document-Frequency` technique implemented in scikit-learn's `TfidfVectorizer`. This method creates the same matrix as the `CountVectorizer` but divides the values for each token in the vocabulary by its frequency across all documents in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.464374Z",
     "start_time": "2019-01-04T17:41:33.776Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer_specific = TfidfVectorizer(vocabulary=specific_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Full vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.466802Z",
     "start_time": "2019-01-04T17:41:33.780Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf = make_pipeline(tfidf_vectorizer, RandomForestClassifier(n_estimators=10,\n",
    "                                                                    random_state=SEED,\n",
    "                                                                    n_jobs=-1))\n",
    "fitted_tfidf = fit_ensemble(pipe_tfidf, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_full = evaluate_ensemble(fitted_tfidf, evaluator, X_test_text, y_test,\n",
    "                                      return_res=True, method=f\"tfidf rf full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.469504Z",
     "start_time": "2019-01-04T17:41:33.785Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_rf_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Only specific vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.471668Z",
     "start_time": "2019-01-04T17:41:33.791Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_specific = make_pipeline(tfidf_vectorizer_specific, RandomForestClassifier(n_estimators=10,\n",
    "                                                                                      random_state=SEED,\n",
    "                                                                                      n_jobs=-1))\n",
    "fitted_tfidf_specific = fit_ensemble(pipe_tfidf_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_rf_specific = evaluate_ensemble(fitted_tfidf_specific, evaluator, X_test_text, y_test,\n",
    "                                          return_res=True, method=f\"tfidf rf specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.473718Z",
     "start_time": "2019-01-04T17:41:33.795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_rf_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.475636Z",
     "start_time": "2019-01-04T17:41:33.802Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc = make_pipeline(tfidf_vectorizer,\n",
    "                               StandardScaler(with_mean=False),\n",
    "                               SVC(gamma=\"auto\",\n",
    "                                   probability=True,\n",
    "                                   random_state=SEED))\n",
    "fitted_tfidf_svc = fit_ensemble(pipe_tfidf_svc, sss, X_train_text, y_train)\n",
    "res_tfidf_svc_full = evaluate_ensemble(fitted_tfidf_svc, evaluator, X_test_text, y_test,\n",
    "                                       return_res=True, method=f\"tfidf svc full v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.477760Z",
     "start_time": "2019-01-04T17:41:33.808Z"
    }
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_svc_full, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.479272Z",
     "start_time": "2019-01-04T17:41:33.813Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_tfidf_svc_specific = make_pipeline(tfidf_vectorizer_specific,\n",
    "                                        StandardScaler(with_mean=False),\n",
    "                                        SVC(gamma=\"auto\",\n",
    "                                            probability=True,\n",
    "                                            random_state=SEED))\n",
    "fitted_tfidf_svc_specific = fit_ensemble(pipe_tfidf_svc_specific, sss, X_train_text, y_train)\n",
    "res_tfidf_svc_specific = evaluate_ensemble(fitted_tfidf_svc_specific, evaluator, X_test_text, y_test,\n",
    "                                           return_res=True, method=f\"tfidf svc specific v{SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.481017Z",
     "start_time": "2019-01-04T17:41:33.817Z"
    }
   },
   "outputs": [],
   "source": [
    "save_pickle(res_tfidf_svc_specific, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect results from running the above with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.483241Z",
     "start_time": "2019-01-04T17:41:33.823Z"
    }
   },
   "outputs": [],
   "source": [
    "list_countvec = read_results(OUTPUT_PATH / \"raw\", \"countvec*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.485615Z",
     "start_time": "2019-01-04T17:41:33.829Z"
    }
   },
   "outputs": [],
   "source": [
    "list_tfidf = read_results(OUTPUT_PATH / \"raw\", \"tfidf*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.488827Z",
     "start_time": "2019-01-04T17:41:33.833Z"
    }
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec = pd.concat([e for l in [list_countvec, list_tfidf] for e in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.491447Z",
     "start_time": "2019-01-04T17:41:33.839Z"
    }
   },
   "outputs": [],
   "source": [
    "results_tfidf_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.493386Z",
     "start_time": "2019-01-04T17:41:33.844Z"
    }
   },
   "outputs": [],
   "source": [
    "save_html(df=results_tfidf_countvec.drop(\"base_method\", axis=1).sort_values([\"mean test auc\", \"mean cv auc\"], axis=0, ascending=False),\n",
    "          name=\"countvec_tfidf_results_default_all_appendix\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarize results and save overview table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.495388Z",
     "start_time": "2019-01-04T17:41:33.849Z"
    }
   },
   "outputs": [],
   "source": [
    "res = results_tfidf_countvec.sort_values([\"mean test auc\", \"mean cv auc\"], axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.497667Z",
     "start_time": "2019-01-04T17:41:33.856Z"
    }
   },
   "outputs": [],
   "source": [
    "res_groupby_method = res.groupby(\"base_method\")\n",
    "res_analysis_table = res_groupby_method.agg({c : [\"mean\", \"std\"] for c in [\"mean train auc\", \"mean cv auc\", \"mean test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.499515Z",
     "start_time": "2019-01-04T17:41:33.862Z"
    }
   },
   "outputs": [],
   "source": [
    "out = res_analysis_table.sort_values([(\"mean test auc\", \"mean\"), (\"mean cv auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.501450Z",
     "start_time": "2019-01-04T17:41:33.867Z"
    }
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"cv auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.503067Z",
     "start_time": "2019-01-04T17:41:33.873Z"
    }
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.505035Z",
     "start_time": "2019-01-04T17:41:33.878Z"
    }
   },
   "outputs": [],
   "source": [
    "save_html(df=out,\n",
    "          name=\"summary_countvec_tfidf_results_default\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained word embeddings + neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is heavily based on these two Kaggle Kernels:\n",
    "- [Processing text when using word embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings/notebook)\n",
    "- [Comparing word embeddings](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T20:17:40.944282Z",
     "start_time": "2019-01-04T20:17:40.912743Z"
    }
   },
   "outputs": [],
   "source": [
    "text_len = data[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "text_para_len = data[\"text\"].apply(lambda x: len(x.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T20:13:08.748274Z",
     "start_time": "2019-01-04T20:13:08.199107Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(vocab)} distinct words in the blog posts.\")\n",
    "print()\n",
    "print(\"Summary statistics on the number of words across all blog posts:\")\n",
    "print(f\"   Mean: {text_len.mean():.2f}\")\n",
    "print(f\"   Median: {text_len.median():.2f}\")\n",
    "print(f\"   Maximum: {text_len.max()}\")\n",
    "print(f\"   Minimum: {text_len.min()}\")\n",
    "print(f\"   Standard deviation: {text_len.std():.2f}\")\n",
    "print()\n",
    "print(\"Summary statistics on the number of paragraphs across all blog posts:\")\n",
    "print(f\"   Mean: {text_para_len.mean():.2f}\")\n",
    "print(f\"   Median: {text_para_len.median():.2f}\")\n",
    "print(f\"   Maximum: {text_para_len.max()}\")\n",
    "print(f\"   Minimum: {text_para_len.min()}\")\n",
    "print(f\"   Standard deviation: {text_para_len.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:38.243165Z",
     "start_time": "2019-01-04T21:30:38.237842Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_LEN = 1000 # max number of words in a blog post\n",
    "EMBEDDING_PATH = Path.cwd() / \"resources\" / \"embeddings\"\n",
    "EMBEDDING_FOLDER = EMBEDDING_PATH / \"glove.840B.300d\"\n",
    "SPECIFIC_ONLY = False\n",
    "L2_REG = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:38.926566Z",
     "start_time": "2019-01-04T21:30:38.922349Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:53.209285Z",
     "start_time": "2019-01-04T21:30:39.271397Z"
    }
   },
   "outputs": [],
   "source": [
    "if (EMBEDDING_FOLDER / \"embeddings_index.pkl\").is_file():\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"rb\") as handle:\n",
    "        embeddings_index = pickle.load(handle)\n",
    "else:\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FOLDER / \"glove.840B.300d.txt\"))\n",
    "    with open(EMBEDDING_FOLDER / \"embeddings_index.pkl\", \"wb\") as handle:\n",
    "        pickle.dump(embeddings_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess the text data to work well with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:53.217292Z",
     "start_time": "2019-01-04T21:30:53.211815Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train_text.copy())\n",
    "X_test = pd.Series(X_test_text.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:53.339636Z",
     "start_time": "2019-01-04T21:30:53.223778Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: x.lower())\n",
    "X_train = X_train.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_train.apply(lambda x: fix_punctuation(x))\n",
    "X_train = X_train.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the same transformations to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:53.425370Z",
     "start_time": "2019-01-04T21:30:53.342394Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = X_test.apply(lambda x: x.lower())\n",
    "X_test = X_test.apply(lambda x: clean_apostrophe(x))\n",
    "X_test = X_test.apply(lambda x: fix_punctuation(x))\n",
    "X_test = X_test.apply(lambda x: fix_specific(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize the texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:53.645898Z",
     "start_time": "2019-01-04T21:30:53.427595Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:53.888304Z",
     "start_time": "2019-01-04T21:30:53.648012Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:30:53.905013Z",
     "start_time": "2019-01-04T21:30:53.890491Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process embeddings into a matrix of size `(max_features, embed_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:04.167828Z",
     "start_time": "2019-01-04T21:30:53.910771Z"
    }
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:04.175611Z",
     "start_time": "2019-01-04T21:31:04.170118Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "if SPECIFIC_ONLY: word_index = {word : i for word, i in word_index.items() if word in specific_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:04.378404Z",
     "start_time": "2019-01-04T21:31:04.178239Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_FEATURES, len(tokenizer.word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:04.385449Z",
     "start_time": "2019-01-04T21:31:04.381053Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "reg = regularizers.l2(L2_REG) if L2_REG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:05.027442Z",
     "start_time": "2019-01-04T21:31:04.388965Z"
    }
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAX_LEN,))\n",
    "x = Embedding(MAX_FEATURES, embed_size, weights=[embedding_matrix], embeddings_regularizer=reg)(inp)\n",
    "x = Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=reg))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T22:20:54.207153Z",
     "start_time": "2018-11-26T22:20:54.193978Z"
    }
   },
   "source": [
    "##### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:05.033903Z",
     "start_time": "2019-01-04T21:31:05.030431Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:49.330717Z",
     "start_time": "2019-01-04T21:31:05.036555Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=16, epochs=10,\n",
    "          validation_data=(X_test, y_test), callbacks=[EarlyStopping(min_delta=0.005, patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:51.988010Z",
     "start_time": "2019-01-04T21:31:49.334845Z"
    }
   },
   "outputs": [],
   "source": [
    "train_auc = metrics.roc_auc_score(y_train, model.predict(X_train))\n",
    "test_auc = metrics.roc_auc_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:52.003555Z",
     "start_time": "2019-01-04T21:31:51.992457Z"
    }
   },
   "outputs": [],
   "source": [
    "results_keras = pd.DataFrame({\"method\": f\"embeddings keras v{SEED}\", \"train auc\": train_auc, \"test auc\": test_auc},\n",
    "                             index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:52.044992Z",
     "start_time": "2019-01-04T21:31:52.008689Z"
    }
   },
   "outputs": [],
   "source": [
    "results_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:31:52.084253Z",
     "start_time": "2019-01-04T21:31:52.050908Z"
    }
   },
   "outputs": [],
   "source": [
    "save_pickle(results_keras, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect results and save for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:38:21.360034Z",
     "start_time": "2019-01-04T21:38:21.333658Z"
    }
   },
   "outputs": [],
   "source": [
    "list_keras = read_results(OUTPUT_PATH / \"raw\", \"*keras*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:38:23.366871Z",
     "start_time": "2019-01-04T21:38:23.358368Z"
    }
   },
   "outputs": [],
   "source": [
    "full_results_keras = pd.concat(list_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:38:26.867248Z",
     "start_time": "2019-01-04T21:38:26.849000Z"
    }
   },
   "outputs": [],
   "source": [
    "full_results_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:38:34.732175Z",
     "start_time": "2019-01-04T21:38:34.715274Z"
    }
   },
   "outputs": [],
   "source": [
    "save_html(df=full_results_keras.drop(\"base_method\", axis=1).sort_values([\"test auc\"], axis=0, ascending=False),\n",
    "          name=\"keras_results_all_appendix\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate results and save for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:40:05.416511Z",
     "start_time": "2019-01-04T21:40:05.399667Z"
    }
   },
   "outputs": [],
   "source": [
    "res = full_results_keras.groupby(\"base_method\").agg({c : [\"mean\", \"std\"] for c in [\"train auc\", \"test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:40:50.216758Z",
     "start_time": "2019-01-04T21:40:50.209564Z"
    }
   },
   "outputs": [],
   "source": [
    "out = res.sort_values([(\"test auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:40:51.815662Z",
     "start_time": "2019-01-04T21:40:51.806349Z"
    }
   },
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:40:54.451906Z",
     "start_time": "2019-01-04T21:40:54.434808Z"
    }
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T21:41:48.894832Z",
     "start_time": "2019-01-04T21:41:48.880949Z"
    }
   },
   "outputs": [],
   "source": [
    "save_html(df=out,\n",
    "          name=\"keras_results_summary\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language model + neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.552596Z",
     "start_time": "2019-01-04T17:41:34.068Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.datasets import URLs\n",
    "from fastai.text import TextList\n",
    "from fastai.basic_data import DatasetType\n",
    "from fastai.text.learner import text_classifier_learner\n",
    "from fastai.text.learner import language_model_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = MODEL_PATH / \"fastai\"\n",
    "if not PATH.is_dir():\n",
    "    PATH.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_N_TOKENS = 0\n",
    "BPTT = 1000\n",
    "MAX_LEN = 2000\n",
    "SPLIT_TEXTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we split the long texts into chunks based on new line characters, train on those and put the results back together in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = pd.DataFrame({\"text\": X_train_text, \"label\": y_train})\n",
    "X_test_full = pd.DataFrame({\"text\": X_test_text, \"label\": y_test})\n",
    "\n",
    "if SPLIT_TEXTS:\n",
    "    X_train_full_exploded = explode_texts(X_train_full)\n",
    "    X_test_full_exploded = explode_texts(X_test_full)\n",
    "\n",
    "    X_train_full_exploded[\"is_test\"] = False\n",
    "    X_test_full_exploded[\"is_test\"] = True\n",
    "\n",
    "    data_proc = pd.concat([X_train_full_exploded, X_test_full_exploded], axis=0)\n",
    "else:\n",
    "    X_train_full[\"is_test\"] = False\n",
    "    X_test_full[\"is_test\"] = True\n",
    "    data_proc = pd.concat([X_train_full, X_test_full], axis=0)\n",
    "    \n",
    "n_tokens = data_proc[\"text\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "print(\"Summary statistics on the number of words across all blog posts:\")\n",
    "print(f\"   Mean: {n_tokens.mean():.2f}\")\n",
    "print(f\"   Median: {n_tokens.median():.2f}\")\n",
    "print(f\"   Maximum: {n_tokens.max()}\")\n",
    "print(f\"   Minimum: {n_tokens.min()}\")\n",
    "print(f\"   Standard deviation: {n_tokens.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Evaluate the Language Model (Metric : Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.557257Z",
     "start_time": "2019-01-04T17:41:34.079Z"
    }
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_df(df=data_proc, path=PATH, cols=\"text\")\n",
    "             .random_split_by_pct(0.2)\n",
    "             .label_for_lm()\n",
    "             .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.559277Z",
     "start_time": "2019-01-04T17:41:34.084Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, bptt=BPTT, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.560873Z",
     "start_time": "2019-01-04T17:41:34.089Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.562759Z",
     "start_time": "2019-01-04T17:41:34.094Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, lr, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.565514Z",
     "start_time": "2019-01-04T17:41:34.099Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save(\"lm_fit_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.567293Z",
     "start_time": "2019-01-04T17:41:34.106Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.load(\"lm_fit_head\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.569270Z",
     "start_time": "2019-01-04T17:41:34.111Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, lr/10, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.571018Z",
     "start_time": "2019-01-04T17:41:34.116Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save(\"lm_fine_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.572991Z",
     "start_time": "2019-01-04T17:41:34.121Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save_encoder(\"lm_fine_tuned_enc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Evaluate the Classifier (Metric = ROC AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = data_proc.copy()[n_tokens >= MIN_N_TOKENS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.578359Z",
     "start_time": "2019-01-04T17:41:34.133Z"
    }
   },
   "outputs": [],
   "source": [
    "data_clf = (TextList.from_df(df=clf_df, path=PATH, cols=[\"text\"], vocab=data_lm.vocab)\n",
    "               .split_from_df(\"is_test\")\n",
    "               .label_from_df(cols=\"label\")\n",
    "               .databunch(bs=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clf.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.580865Z",
     "start_time": "2019-01-04T17:41:34.141Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clf, bptt=BPTT, max_len=MAX_LEN, drop_mult=0.3)\n",
    "learn.load_encoder(\"lm_fine_tuned_enc\")\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.582470Z",
     "start_time": "2019-01-04T17:41:34.146Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.584463Z",
     "start_time": "2019-01-04T17:41:34.152Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, lr, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.586445Z",
     "start_time": "2019-01-04T17:41:34.159Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_first_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.588692Z",
     "start_time": "2019-01-04T17:41:34.165Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_first_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.590833Z",
     "start_time": "2019-01-04T17:41:34.173Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(lr/(2.6**4), lr), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.592505Z",
     "start_time": "2019-01-04T17:41:34.177Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_second_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.594106Z",
     "start_time": "2019-01-04T17:41:34.184Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_second_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.595972Z",
     "start_time": "2019-01-04T17:41:34.189Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice((lr/5)/(2.6**4), lr/5), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.598241Z",
     "start_time": "2019-01-04T17:41:34.196Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save(\"clf_third_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.600905Z",
     "start_time": "2019-01-04T17:41:34.202Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.load(\"clf_third_stage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.603695Z",
     "start_time": "2019-01-04T17:41:34.208Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice((lr/10)/(2.6**4), lr/10), moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_test = clf_df.loc[clf_df.is_test == True, \"text\"].apply(lambda t: learn.predict(t)[2].numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_train = clf_df.loc[clf_df.is_test == False, \"text\"].apply(lambda t: learn.predict(t)[2].numpy()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T17:41:38.606682Z",
     "start_time": "2019-01-04T17:41:34.214Z"
    }
   },
   "outputs": [],
   "source": [
    "#This doesn't seem to produce results consistent with what we get when explicitly calling predict on each sample\n",
    "#I do need to finish this assignment before looking into this more closely\n",
    "#Thus, I will use the slower version above for now\n",
    "#p1_test = learn.get_preds()[0][:, 1].numpy()\n",
    "#p1_train = learn.get_preds(ds_type=DatasetType.Train)[0][:, 1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_TEXTS:\n",
    "    print(\"TRAIN PERFORMANCE\")\n",
    "    evaluate_exploded(clf_df[clf_df.is_test == False], p1_train, y_train)\n",
    "    print()\n",
    "    print(\"TEST PERFORMANCE\")\n",
    "    evaluate_exploded(clf_df[clf_df.is_test == True], p1_test, y_test)\n",
    "else:\n",
    "    print(\"TRAIN PERFORMANCE\")\n",
    "    train_auc = metrics.roc_auc_score(clf_df.loc[clf_df.is_test == False, \"label\"], p1_train)\n",
    "    print(f\"AUC: {train_auc:.2f}\")\n",
    "    print()\n",
    "    print(\"TEST PERFORMANCE\")\n",
    "    test_auc = metrics.roc_auc_score(clf_df.loc[clf_df.is_test == True, \"label\"], p1_test)\n",
    "    print(f\"AUC: {test_auc:.2f}\")\n",
    "    results_lm_finetune = pd.DataFrame({\"method\": f\"lm fine tuning v{SEED}\", \"train auc\": train_auc, \"test auc\": test_auc},\n",
    "                         index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lm_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(results_lm_finetune, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect results and save for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lm_finetune = read_results(OUTPUT_PATH, \"*lm*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_lm_finetune = pd.concat(list_lm_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_lm_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_html(df=full_results_lm_finetune.drop(\"base_method\", axis=1).sort_values([\"test auc\"], axis=0, ascending=False),\n",
    "          name=\"lm_finetune_results_all_appendix\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize results and save summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = full_results_lm_finetune.groupby(\"base_method\").agg({c : [\"mean\", \"std\"] for c in [\"train auc\", \"test auc\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = res.sort_values([(\"test auc\", \"mean\")], axis=0, ascending=False)\n",
    "out.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[\"train auc\", \"test auc\"], [\"mean\", \"std\"]])\n",
    "out.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_html(df=out,\n",
    "          name=\"lm_finetune_results_summary\",\n",
    "          out=OUTPUT_PATH,\n",
    "          index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
